[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lorenzo Posti's Machine Learning Quarto blog",
    "section": "",
    "text": "Intro to diffusion models from scratch\n\n\n\n\n\n\n\nneural_network\n\n\ndiffusion\n\n\njupyter\n\n\n\n\nThis notebook is an introduction to diffusion models that are currently state-of-the-art in computer vision and in image generation. I build from scratch a diffusion model able to generate realistic rotation curves starting from purely random noise.\n\n\n\n\n\n\nJan 13, 2023\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nvcdisk: a python package for galaxy rotation curves\n\n\n\n\n\n\n\njupyter\n\n\nvcdisk\n\n\n\n\nThis notebook presents a minimal python package that I wrote to solve Poisson’s equation in galactic disks. vcdisk is an handy toolbox of essential functions to compute the circular velocity of thick disks and flattened bulges of arbitrary surface density.\n\n\n\n\n\n\nDec 8, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nA simple neural network to predict galaxy rotation curves from photometry\n\n\n\n\n\n\n\nneural_network\n\n\njupyter\n\n\n\n\nDesigning a simple feedforward neural network, made with just linear layers, activations, and batch normalization, to predict rotation curves from galaxy surface brightness profiles. Comparison to the MOND empirical law.\n\n\n\n\n\n\nNov 16, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nRotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results\n\n\n\n\n\n\n\ngaussian_processes\n\n\n\n\nCorrelations between velocity measurements in disk galaxy rotation curves are usually neglected when fitting dynamical models. This notebook, which accompanies the paper Posti (2022), Res. Notes AAS, 6, 233, shows how data correlations can be taken into account in rotation curve decompositions using Gaussian Processes.\n\n\n\n\n\n\nNov 2, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Processes: modelling correlated noise in a dataset\n\n\n\n\n\n\n\ngaussian_processes\n\n\nbayesian\n\n\njupyter\n\n\n\n\nIndependent datapoints are most often just a convenient idealisation which can even hamper your model inference at times and bias your results. Learn how to embrace the reality of correlated noise in the data and marginalize the parameter posteriors with Gaussian Processes.\n\n\n\n\n\n\nOct 17, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nVariational Autoencoder: learning an underlying distribution and generating new data\n\n\n\n\n\n\n\nneural_network\n\n\nautoencoder\n\n\nvariational autoencoder\n\n\nbasics\n\n\njupyter\n\n\n\n\nConstructing an autoencoder that learns the underlying distribution of the input data, generated from a multi-dimensional smooth function f=f(x_1,x_2,x_3,x_4). This can be used to generate new data, sampling from the learned distribution\n\n\n\n\n\n\nOct 7, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nAutoencoder represents a multi-dimensional smooth function\n\n\n\n\n\n\n\nneural_network\n\n\nautoencoder\n\n\nbasics\n\n\njupyter\n\n\n\n\nSetting up a simple Autoencoder neural network to reproduce a dataset obtained by sampling a multi-dimensional smooth function f=f(x_1,x_2,x_3,x_4). As an example I’m using a disc+halo rotation curve model where both components are described by 2-parameters circular velocities\n\n\n\n\n\n\nJun 10, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround-up construction of a simple neural network\n\n\n\n\n\n\n\nneural_network\n\n\nbasics\n\n\njupyter\n\n\n\n\nConstructing a simple multi-layered neural network (NN) from scratch using pure python and a bit of pytorch. This is mostly my personal re-writing of the fantastic lesson 1 of the fast.ai course Part 2\n\n\n\n\n\n\nMar 22, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding how basic linear NNs handle non-linearities\n\n\n\n\n\n\n\nneural_network\n\n\nbasics\n\n\nbayesian\n\n\nMCMC\n\n\njupyter\n\n\n\n\nA deep exploration, an visualization, of how a single-layer linear neural network (NN) is able to approximate non linear behaviours with a just handful of neurons and the magic of activation functions.\n\n\n\n\n\n\nMar 18, 2022\n\n\nLorenzo Posti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about_blog.html#an-astronomers-perspective-on-deep-learning",
    "href": "about_blog.html#an-astronomers-perspective-on-deep-learning",
    "title": "Lorenzo Posti's Machine Learning Quarto blog",
    "section": "An Astronomer’s perspective on Deep Learning",
    "text": "An Astronomer’s perspective on Deep Learning\nThis blog contains notes and notebooks of a researcher in Astronomy diving deep into the study of deep learning. What I found during my quest, and what I report here, is meant to be purely anecdotical and it is not necessarily following a logical and sound educational path. The hope, with which I build this blog, is that it will eventually become a useful reference for me to look back at and that also other people, with a similar background to mine, may find some use out of these notes and notebooks.\nMy name is Lorenzo Posti and I’m currently a postdoctoral researcher at the Strasbourg Observatory (France) specialized in models of galaxy dynamics, which I use to study the origin of galaxies and the nature of dark matter. Find my list of publications at ADS\nThis site is built using Quarto. The diagram of the neural network in the banner was made using NN-SVG, the input image shows isophotal contours of the galaxy NGC 4342 rendered from Hubble observations."
  },
  {
    "objectID": "lop.html",
    "href": "lop.html",
    "title": "List of Publications",
    "section": "",
    "text": "Metrics (source: ADS)\n\n38 refereed publications, 13 first-author, 12 second-author\nh-index: 17\nCitations: 968\n\n\n\nDi Teodoro, E. M., Posti, L., Fall, S. M., et al. 2023, MNRAS, 518, 6340, 10.1093/mnras/stac3424\n\n\nDi Teodoro, E. M., Posti, L., Ogle, P. M., Fall, S. M., & Jarrett, T. 2021, MNRAS, 507, 5820, 10.1093/mnras/stab2549\n\n\nHagen, J. H. J., Helmi, A., de Zeeuw, P. T., & Posti, L. 2019, A&A, 629, A70, 10.1051/0004-6361/201935264\n\n\nMancera Piña, P. E., Fraternali, F., Adams, E. A. K., et al. 2019, ApJL, 883, L33, 10.3847/2041-8213/ab40c7\n\n\nMancera Piña, P. E., Fraternali, F., Oman, K. A., et al. 2020, MNRAS, 495, 3636, 10.1093/mnras/staa1256\n\n\nMancera Piña, P. E., Posti, L., Fraternali, F., Adams, E. A. K., & Oosterloo, T. 2021a, A&A, 647, A76, 10.1051/0004-6361/202039340\n\n\nMancera Piña, P. E., Posti, L., Pezzulli, G., et al. 2021b, A&A, 651, L15, 10.1051/0004-6361/202141574\n\n\nMarasco, A., Cresci, G., Posti, L., et al. 2021, MNRAS, 507, 4274, 10.1093/mnras/stab2317\n\n\nMarasco, A., Fraternali, F., Posti, L., et al. 2019, A&A, 621, L6, 10.1051/0004-6361/201834456\n\n\nMarasco, A., Posti, L., Oman, K., et al. 2020, A&A, 640, A70, 10.1051/0004-6361/202038326\n\n\nMassari, D., Breddels, M. A., Helmi, A., et al. 2018, Nature Astronomy, 2, 156, 10.1038/s41550-017-0322-y\n\n\nMassari, D., Posti, L., Helmi, A., Fiorentino, G., & Tolstoy, E. 2017, A&A, 598, L9, 10.1051/0004-6361/201630174\n\n\nMüller, O., Durrell, P. R., Marleau, F. R., et al. 2021, ApJ, 923, 9, 10.3847/1538-4357/ac2831\n\n\nMüller, O., Ibata, R., Rejkuba, M., & Posti, L. 2019, A&A, 629, L2, 10.1051/0004-6361/201936392\n\n\nNipoti, C., & Posti, L. 2013, MNRAS, 428, 815, 10.1093/mnras/sts070\n\n\nNipoti, C., & Posti, L. 2014, ApJ, 792, 21, 10.1088/0004-637X/792/1/21\n\n\nNipoti, C., Posti, L., Ettori, S., & Bianconi, M. 2015, Journal of Plasma Physics, 81, 495810508, 10.1017/S0022377815000781\n\n\nOria, P.-A., Famaey, B., Thomas, G. F., et al. 2021, ApJ, 923, 68, 10.3847/1538-4357/ac273d\n\n\nPascale, R., Binney, J., Nipoti, C., & Posti, L. 2019, MNRAS, 488, 2423, 10.1093/mnras/stz1617\n\n\nPascale, R., Posti, L., Nipoti, C., & Binney, J. 2018, MNRAS, 480, 927, 10.1093/mnras/sty1860\n\n\nPosti, L. 2022b (Zenodo; Zenodo), 10.5281/zenodo.7294885\n\n\nPosti, L. 2022a, Research Notes of the American Astronomical Society, 6, 233, 10.3847/2515-5172/aca0df\n\n\nPosti, L., Binney, J., Nipoti, C., & Ciotti, L. 2015, MNRAS, 447, 3060, 10.1093/mnras/stu2608\n\n\nPosti, L., & Fall, S. M. 2021, A&A, 649, A119, 10.1051/0004-6361/202040256\n\n\nPosti, L., Famaey, B., Pezzulli, G., et al. 2020, A&A, 644, A76, 10.1051/0004-6361/202038474\n\n\nPosti, L., Fraternali, F., Di Teodoro, E. M., & Pezzulli, G. 2018a, A&A, 612, L6, 10.1051/0004-6361/201833091\n\n\nPosti, L., Fraternali, F., & Marasco, A. 2019a, A&A, 626, A56, 10.1051/0004-6361/201935553\n\n\nPosti, L., & Helmi, A. 2019, A&A, 621, A56, 10.1051/0004-6361/201833355\n\n\nPosti, L., Helmi, A., Veljanoski, J., & Breddels, M. A. 2018b, A&A, 615, A70, 10.1051/0004-6361/201732277\n\n\nPosti, L., Marasco, A., Fraternali, F., & Famaey, B. 2019b, A&A, 629, A59, 10.1051/0004-6361/201935982\n\n\nPosti, L., Nipoti, C., Stiavelli, M., & Ciotti, L. 2014, MNRAS, 440, 610, 10.1093/mnras/stu301\n\n\nPosti, L., Pezzulli, G., Fraternali, F., & Di Teodoro, E. M. 2018c, MNRAS, 475, 232, 10.1093/mnras/stx3168\n\n\nPosti, L., van de Ven, G., Binney, J., Nipoti, C., & Ciotti, L. 2016, in The interplay between local and global processes in galaxies, ed. S. F. Sanchez, C. Morisset, & G. Delgado-Inglada, 39\n\n\nSavino, A., & Posti, L. 2019, A&A, 624, L9, 10.1051/0004-6361/201935417\n\n\nSkúladóttir, Á., Salvadori, S., Amarsi, A. M., et al. 2021, ApJL, 915, L30, 10.3847/2041-8213/ac0dc2\n\n\nTortora, C., Posti, L., Koopmans, L. V. E., & Napolitano, N. R. 2019, MNRAS, 489, 5483, 10.1093/mnras/stz2320\n\n\nVeljanoski, J., Helmi, A., Breddels, M., & Posti, L. 2019, A&A, 621, A13, 10.1051/0004-6361/201732303\n\n\nZhu, L., van de Ven, G., Watkins, L. L., & Posti, L. 2016, MNRAS, 463, 1117, 10.1093/mnras/stw2081"
  },
  {
    "objectID": "posts/2022-11-16-rotcurves_nn_mond.html",
    "href": "posts/2022-11-16-rotcurves_nn_mond.html",
    "title": "A simple neural network to predict galaxy rotation curves from photometry",
    "section": "",
    "text": "An interesting, and still open, question in modern Astronomy is how the properties of a galaxy, such as its luminosity, size, and stellar/gas content, shape the gravitational field in which the galaxy is immersed. In the case of disk galaxies, orbits are close to circular, thus measuring the rotational velocity of stars or gas in the disk gives a direct probe of the total gravitational potential. For the galaxies for which we are able to obtain such rotation curves with optical or radio telescopes, we can try to establish a link between the shape of the circular velocity curve, i.e. of the total gravitational potential, and with the distribution of luminous baryons, stars and gas.\nIt is very well-known that the gravitational field generated by the luminous baryons is not enough to account for the observed rotation curves – the so-called missing mass problem. This could be either because there is an additional invisible matter component to be considered in the potential budget (dark matter) or because the laws of gravity as we know them need revision in the context of disk galaxies. Whathever the reason, the missing mass problem implies that by simply measuring the surface brightness profile of a disk galaxy there is no simple, direct way to transform this into a circular velocity profile.\nIn this post I will try to establish a connection between the distribution of luminous mass of disk galaxies, that can be derived with cheap photometric observations, and the shape of the rotation curve, which traces the gravitational potential. This could be useful in order to predict the shape of the gravitational field with just images of the galaxy.\nTo do this I will train a very simple feedforward neural network (NN) made with just one hidden linear layer, activations, and batch normalization. This will be trained with a sample of 145 galaxies with measured rotation curves in the SPARC database.\nThe baseline model to which I will compare the performance of the NN is the framework of Modified Newtonian Dynamics (MOND), since its empirical law that relates the baryonic acceleration to the total gravitational acceleration is observed to provide excellent approximations of the observed rotation curves (e.g. Begeman et al. 1991).\n\n\nI use the rich rotation curve catalog called SPARC, which was originally compiled by Lelli et al. (2016) and it is perfectly suited for this kind of study.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport torch, math\nimport torch.nn as nn\nfrom torch import tensor\n\n%matplotlib inline\n%config Completer.use_jedi = False\n\nrng = np.random.default_rng()\n\n\nHere I copied the table describing the sample and the table with all the velocity profiles and surface brightnesses.\n\n# mastertable\ndf_sp_master = pd.read_csv('sparc_mastertable.txt', delim_whitespace=True, header=None,\n                           names=['Galaxy','T','D','e_D','f_D','Inc','e_Inc','L[3.6]','e_L[3.6]','Reff','SBeff',\n                                  'Rdisk','SBdisk0','MHI','RHI','Vflat','e_Vflat','Q','Ref.'])\n\n# profiles table\ndf_sp_mm = pd.read_csv('sparc_mmtab.txt', delim_whitespace=True)\n\nThe SPARC database provides Vgas, Vdisk, and Vbul for each radius R, which are the contribution of respectively the gas, disk stellar component, and bulge stellar component to the total circular velocity curve of the galaxy. This are obtained directly from observations of the gas/disk/bulge surface density profiles through Poisson’s equation (see Casertano 1983).\nFrom the contributions to the circular velocities of the various components, we can obtain the total baryonic circular velocity which is\n\\[\nV_{\\rm bar}^2 = V_{\\rm gas}^2 + (M/L)_{\\rm disk}V_{\\rm disk}^2 + (M/L)_{\\rm bul}V_{\\rm bul}^2,\n\\]\nwhere we set \\((M/L)_{\\rm disk}=0.5\\) and \\((M/L)_{\\rm bul}=0.7\\) (at 3.6\\(\\mu\\)m) as suggested by Lelli et al. (2016).\n\ndf_sp_mm['Vbar'] = np.sqrt(df_sp_mm['Vgas']**2+0.5*df_sp_mm['Vdisk']**2+0.7*df_sp_mm['Vbul']**2)\n\n\n\n\nThe best way that we have to predict rotation curves from surface density profiles is by using the empirical MOND law. Modified Newtonian Dynamics (MOND) was introduced by Milgrom (1983) and it consists of a modification of Newtonian gravity which is empirically observed to fit galaxy rotation curves without the need for dark matter.\nWhile MOND has been developed to be a full gravitational theory, here I am going to use the phenomenological law of gravity relevant for galaxy disks (see e.g. Eq. 3 here). The acceleration in MOND is related to the Newtonian acceleration as: \\[\ng_{\\rm MOND} = g_{\\rm N}\\nu(g_{\\rm N}/a_0), \\qquad {\\rm or} \\qquad g_{\\rm N} = g_{\\rm MOND}\\mu(g_{\\rm MOND}/a_0),\n\\] where \\(g_{\\rm N}\\) is the Newtonian acceleration, \\(a_0\\) is a constant acceleration and \\(\\nu\\) is the so-called interpolating function that specifies the MOND theory. Here I am using \\[\n\\nu(y) = \\frac{1+(1+4y^{-1})^{1/2}}{2},\n\\] whose inverse is \\(\\mu(x)=x/(1+x)\\), i.e. the so-called simple interpolating function in MOND jargon (Famaey & Binney 2005, Famaey & McGaugh 2012), and a constant acceleration scale \\(a_0 = 1.2\\times 10^{-10} \\rm m\\,s^{-2}\\) (see e.g. McGaugh et al. 2016).\nThe acceleration due to baryons alone is just \\(g_{\\rm bar} = V_{\\rm bar}^2/R\\). Then, since \\(V_{\\rm MOND} = \\sqrt{g_{\\rm MOND}R}\\) and \\(g_{\\rm MOND} = g_{\\rm bar}\\nu(g_{\\rm bar}/a_0)\\), I finally get that \\[\nV_{\\rm MOND} = \\sqrt{R\\,g_{\\rm bar}\\nu(g_{\\rm bar}/a_0)}=V_{\\rm bar}\\sqrt{\\nu(g_{\\rm bar}/a_0)}\n\\]\n\ndf_sp_mm['Gbar'] = df_sp_mm['Vbar']**2 / (df_sp_mm['R']*3.086e16) * 1e3 # m s^-2\n\n# interpolating nu-function (Eq. 3, http://www.scholarpedia.org/article/The_MOND_paradigm_of_modified_dynamics)\n# corresponding to the \"simple\" mu-function of Famaey & Binney\nnu_mond = lambda y: 0.5+0.5*np.sqrt(1+4/y)\n\ndf_sp_mm['Vmond'] = df_sp_mm['Vbar'] * np.sqrt(nu_mond(df_sp_mm['Gbar']/1.2e-10))\n\nNow we have a baseline model that we can use as an expectation on the rotation curve of a disk galaxy from its surface brightness"
  },
  {
    "objectID": "posts/2022-11-16-rotcurves_nn_mond.html#a-neural-network-model",
    "href": "posts/2022-11-16-rotcurves_nn_mond.html#a-neural-network-model",
    "title": "A simple neural network to predict galaxy rotation curves from photometry",
    "section": "A neural network model",
    "text": "A neural network model\nWe now want to set up a deep neural network (NN) architecture that gets as an input arrays of \\(R\\) and \\(V_{\\rm bar}(R)\\) and outputs arrays of the total circular velocity of a disk galaxy, \\(V_{\\rm obs}(R)\\).\n\nData manipulation\nLet’s start by splitting the sample into training and validation sets. There are 175 galaxies in SPARC, each with a rotation curve array with variable size. Let’s split the catalog in 145 random galaxies for training and 30 random galaxis for validation, even though they will have very different numbers of curve datapoints.\n\n# using rng.choice to generate random integers without repetitions\nvalid_gals = rng.choice(174, size=30, replace=False) # 30/175 = 17% of galaxies in validation set\ntrain_idx = np.where(~np.isin(df_sp_mm.Galaxy, df_sp_master['Galaxy'][valid_gals]))[0]\nvalid_idx = np.where(np.isin(df_sp_mm.Galaxy, df_sp_master['Galaxy'][valid_gals]))[0]\n\n# shuffle training dataset\nrng.shuffle(train_idx)\n\nsplits = (list(train_idx), list(valid_idx))\n\nThen I normalize the \\(X=[R, V_{\\rm bar}]\\) and \\(Y=[V_{\\rm obs}]\\) arrays, i.e. the input and output of the NN, and I convert them to tensors.\n\ndf_nn = df_sp_mm[['R', 'Vbar']] # columns used in the NN\ndf_nn_train, df_nn_valid = df_nn.iloc[train_idx], df_nn.iloc[valid_idx]\ny_nn_train, y_nn_valid = df_sp_mm['Vobs'].iloc[train_idx], df_sp_mm['Vobs'].iloc[valid_idx]\n\n# normalize X\nmean_df_nn, std_df_nn = df_nn_train.mean(), df_nn_train.std()\ndf_nn_train_norm = (df_nn_train-mean_df_nn)/std_df_nn\ndf_nn_valid_norm = (df_nn_valid-mean_df_nn)/std_df_nn\n# normalize y\nmean_y_nn, std_y_nn = y_nn_train.mean(), y_nn_train.std()\ny_nn_train_norm = (y_nn_train-mean_y_nn)/std_y_nn\ny_nn_valid_norm = (y_nn_valid-mean_y_nn)/std_y_nn\n\n# convert to tensors\nx_train = tensor(df_nn_train_norm.values).float()\ny_train = tensor(y_nn_train_norm.values).float()\nx_valid = tensor(df_nn_valid_norm.values).float()\ny_valid = tensor(y_nn_valid_norm.values).float()\n\nNormalize and tensorize also the MOND predictions, so that we will be able to compare the losses with the NN model.\n\nymond_train = tensor(((df_sp_mm['Vmond'].iloc[train_idx]-mean_y_nn)/std_y_nn).values).float()\nymond_valid = tensor(((df_sp_mm['Vmond'].iloc[valid_idx]-mean_y_nn)/std_y_nn).values).float()\n\n\nMinimal Dataset and Dataloader\nI bundle the training and validation sets into a simple dataset class derived from torch.utils.data.Dataset. This is meant to be used with torch.utils.data.DataLoader for batch training.\nDefining the datasets\n\nclass RotCurveDset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __len__(self): return len(self.x)\n        \n    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n    \ndset_train = RotCurveDset(x_train, y_train)\ndset_valid = RotCurveDset(x_valid, y_valid)\n\nand defining the dataloaders, with a rather large batch size (640) compared to the total number of the training datapoints (~2700)\n\nbs = 64*10\ndload_train = torch.utils.data.DataLoader(dset_train, batch_size=bs)\ndload_valid = torch.utils.data.DataLoader(dset_valid, batch_size=bs)\n\n\n\n\nThe torch.nn model\nI set up an extremely simple architecture with just one hidden layer, with 64 units, a leaky ReLU activation, and a BatchNorm layer.\n\nnl1 = 64\nmodel = nn.Sequential(nn.Linear(2, nl1), nn.LeakyReLU(), nn.BatchNorm1d(nl1),\n#                       nn.Linear(nl1, nl1), nn.LeakyReLU(), #nn.Dropout1d(p=0.6),\n                      nn.Linear(nl1, 1))\nmodel\n\nI use a standard mean squared error loss function and the Adam optimizer algorithm, with a moderate weight decay to prevent overfitting.\n\nloss_fn = nn.MSELoss(reduction='mean')\noptim   = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n\n\n\nTraining loop\nWe now train the model. At each epoch we store the training loss and validation loss and after 1000 epochs we compare the training losses and validation losses to the MOND case.\n\n%%time\nepochs = 1000\n\ntrain_losses, valid_losses = [], []\nfor i in range(epochs):\n\n    #\n    # train phase\n    #\n    model.train()\n    for batch, (x,y) in enumerate(dload_train):\n        # model and loss\n        y_pred = model(x)\n        loss = loss_fn(y_pred.squeeze(), y)\n        \n        # backprop\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    \n    #\n    # test phase\n    #\n    model.eval()\n    with torch.no_grad():\n        loss = loss_fn(model(x_train).squeeze(), y_train)\n        val_loss = loss_fn(model(x_valid).squeeze(), y_valid)\n        train_losses.append(loss)\n        valid_losses.append(val_loss)\n    if i % 100 == 0: print (\"Epoch: %2d   [Train Loss: %1.5f  //  Valid Loss: %1.5f]\" % (i, loss, val_loss))\n        \nprint ()\nprint ('---------------------')\nprint ('Comparison with MOND:  [Train Loss: %1.5f  //  Valid Loss: %1.5f]' % (loss_fn(ymond_train, y_train),\n                                                                              loss_fn(ymond_valid, y_valid)))\nprint ('---------------------')\nprint ()\n\nEpoch:  0   [Train Loss: 1.05912  //  Valid Loss: 1.13404]\nEpoch: 100   [Train Loss: 0.09393  //  Valid Loss: 0.07471]\nEpoch: 200   [Train Loss: 0.07501  //  Valid Loss: 0.05540]\nEpoch: 300   [Train Loss: 0.07064  //  Valid Loss: 0.05421]\nEpoch: 400   [Train Loss: 0.06949  //  Valid Loss: 0.05205]\nEpoch: 500   [Train Loss: 0.06878  //  Valid Loss: 0.05173]\nEpoch: 600   [Train Loss: 0.06816  //  Valid Loss: 0.05136]\nEpoch: 700   [Train Loss: 0.06742  //  Valid Loss: 0.05121]\nEpoch: 800   [Train Loss: 0.06701  //  Valid Loss: 0.05139]\nEpoch: 900   [Train Loss: 0.06668  //  Valid Loss: 0.05140]\n\n---------------------\nComparison with MOND:  [Train Loss: 0.07238  //  Valid Loss: 0.04494]\n---------------------\n\nCPU times: user 54 s, sys: 1.24 s, total: 55.3 s\nWall time: 42 s\n\n\nWe can see that while the NN model is able to have a training loss significantly smaller than the MOND case, the validation loss is similar or even smaller in the MOND case. This exemplifies the difficulty in generalizing the behaviour learned in the training phase.\nLet’s now look at the plot of the training and validation losses as a function of epoch.\n\nplt.loglog(train_losses, label='train')\nplt.loglog(valid_losses, label='valid')\nplt.legend();\nplt.xlabel('epoch')\nplt.ylabel('loss');\n\n\n\n\nHere we can see that for this particular train/valid split, the learning rate and the weight decay hyper-parameters, as well as the number of units and linear layers of the NN, are well tuned to have gradually decreasing train and validation losses. Increasing the model’s complexity or adding additional layers (e.g. Dropout) does not seem to help in this case, thus I preferred to stick with a simple 1-layer feedforward NN.\n\n\nComparing rotation curve predictions\nNow that we have trained the NN model, we can apply it to the validation set in order to make proper predictions for the rotation curves of disk galaxies just from their surface density profile.\n\ny_pred = model(x_valid).squeeze(-1).detach()\n\nAnd finally we can plot the rotation curves of the 30 galaxies in the validation set in separate panels. Below I overplot the real data (black points), the MOND empirical expectation (blue dotted curve), and the NN prediction (orange solid curve)\n\nfig, ax = plt.subplots(figsize=(18,20), ncols=5, nrows=6, gridspec_kw={'hspace':0.3})\n\nfor k,g in enumerate(df_sp_mm['Galaxy'].iloc[valid_idx].unique()):\n    \n    i,j = int(k/5), k%5\n\n    ax[i,j].errorbar(df_sp_mm['R'][df_sp_mm['Galaxy']==g], df_sp_mm['Vobs'][df_sp_mm['Galaxy']==g], \n                     yerr=df_sp_mm['e_Vobs'][df_sp_mm['Galaxy']==g], c='k', fmt='.', lw=0.5, label='data')\n    ax[i,j].plot(df_sp_mm['R'][df_sp_mm['Galaxy']==g], df_sp_mm['Vmond'][df_sp_mm['Galaxy']==g], c='C9', \n                 ls=':', label='MOND')\n    ax[i,j].plot(df_sp_mm['R'][df_sp_mm['Galaxy']==g], \n                 std_y_nn*y_pred[(df_sp_mm['Galaxy'].iloc[valid_idx]==g).values] + mean_y_nn, c='C1',\n                 lw=1, label='NN')\n    ax[i,j].set_xlim((0,None))\n    ax[i,j].set_ylim((0,None))\n    ax[i,j].set_title(g)\n    if i==5: ax[i,j].set_xlabel('radius / kpc');\n    if j==0: ax[i,j].set_ylabel('velocity / km s'+'$^{-1}$');\nax[0,0].legend();\n\n\n\n\nAll in all, the performance of the NN model are quite satisfactory as it gives nice predictions for \\(V_{\\rm obs}\\) for the majority, though not all, galaxies.\nStrikingly, the predictions of the NN model are extremely close to the MOND rotation curves! This means that a flexible NN when asked to predict rotation curves from surface densities reduces to something that closely resemble the MOND phenomenology. This suggests that, at least in the SPARC dataset, there is nothing more to learn on the rotation curve shapes that is not encapsulated in the MOND empirical law.\nTo see this even better, let’s plot the baryonic acceleration versus the total gravitational acceleration oberved/predicted in the rotation curves. This is the so-called radial acceleration relation plot (McGaugh et al. 2016).\nThe MOND prediction is pretty clear on this diagram, as \\(g_{\\rm MOND} = g_{\\rm bar} \\nu(g_{\\rm bar}/a_0)\\). On the other hand, to generate this for the NN model I take a toy rotation curve that is flat at 200 km/s and that spans a huge radial radial extension, from 0.1 kpc to 1 Mpc. This is done just to cover smoothly both the low- and high-acceleration regimes in which the SPARC galaxies are found.\n\ndef gmond(vbar, r): return (vbar**2 * nu_mond(vbar**2/(r*3.086e16)*1e3/1.2e-10)) / (r*3.086e16)*1e3\ndef get_normed_rv_tensor(r, v):\n    return ((torch.vstack((tensor(r),tensor(v))).T - tensor(mean_df_nn.values))/tensor(std_df_nn.values)).float()\n\nr = np.logspace(-1., 3., 200)\nv = np.full_like(_r, 200.)\nvmod = (model(get_normed_rv_tensor(r, v)).squeeze(-1).detach()*std_y_nn+mean_y_nn).numpy()\n\n\nfig,ax = plt.subplots(figsize=(7,7))\nax.scatter(df_sp_mm['Gbar'], df_sp_mm['Vobs']**2 / (df_sp_mm['R']*3.086e16) * 1e3, s=2, \n           c='grey', alpha=0.5, label='data')\nax.loglog([1e-12,1e-8],[1e-12,1e-8], 'k-', lw=0.5, label='1:1')\nax.loglog(v**2/(r*3.086e16)*1e3, gmond(v, r), ':', c='C9', lw=6, label='MOND')\nax.loglog(v**2/(r*3.086e16)*1e3, vmod**2/(r*3.086e16)*1e3, '-', c='C1', lw=2, label='NN')\nax.tick_params(labelsize=14)\nax.legend(fontsize=14)\nax.set_xlabel(r\"$g_{\\rm bar}/\\rm m\\,s^{-2}$\", fontsize=20);\nax.set_ylabel(r\"$g_{\\rm obs}/\\rm m\\,s^{-2}$\", fontsize=20);\n\n\n\n\nThere is clearly a very good agreement between both models and the data and between the NN and MOND models themselves. Both the data and the two models follow a smooth departure from the 1:1 line at around the charcteristic acceleration scale \\(a_0=1.2\\times 10^{-10}\\rm m\\,s^{-2}\\).\nThe fact that the flexible NN model follows closely the MOND expectations in this diagram is, again, suggesting that the full SPARC dataset can be very well described with just the MOND empirical law plus some scatter."
  },
  {
    "objectID": "posts/2022-12-08-vcdisk.html",
    "href": "posts/2022-12-08-vcdisk.html",
    "title": "vcdisk: a python package for galaxy rotation curves",
    "section": "",
    "text": "vcdisk is a handy toolbox which implements several essential routines for anyone working on galactic dynamics and, in particular, with galaxy rotation curves. vcdisk provides fast and efficient solutions of Poisson’s equation (using results from Casertano 1983, Cuddeford 1993, Noordermeer 2008 and Binney & Tremaine 2008) in the context of thick galaxy disks and flattened oblate bulges of any observed surface density profile.\nIt is a new minimial python package that I wrote and it can be found at https://github.com/lposti/vcdisk. This notebook reproduces the introductory “How to use vcdisk” tutorial which can be found in the documentation.\n\n\nThe most typical use case for vcdisk is in rotation curve modelling. Specifically, this module allows to compute the contribution to the circular velocity of the observed baryonic components of a disk galaxy, e.g. the stellar disk, the gas disk, the stellar bulge.\nIf we have some kinematical observations that allowed us to measure the rotational velocities of some material in circular orbits in the disk of a spiral galaxy (e.g. cold gas), the observed rotation curve can be decomposed into several dynamically important components in the galaxy  where \\(V_{\\rm DM}\\) is the contribution from dark matter, \\(V_{\\star,\\rm disk}\\) from the stellar disk, \\(V_{\\rm gas, disk}\\) from the gas disk etc. While the left-hand side of this equation usually comes from observations, and the contribution of DM is the unknown that is often what we want to constrain, for all the other terms there is vcdisk!\nvcdisk calculates the contribution to the gravitational field of an axisymmetric component whose radial surface density distribution is known. For instance, the surface density profiles of stellar (\\(\\Sigma_{\\star, \\rm disk}\\)) and gaseous disks (\\(\\Sigma_{\\star, \\rm disk}\\)) can be directly obtained from galaxy images in appropriate wavebands and these can then be used as input for vcdisk.vcdisk to calculate \\(V_{\\rm disk}(R, \\Sigma_{\\rm disk}(R))\\).\nThe vcdisk module provides appropriate calculations for both disky (vcdisk.vcdisk) and spheroidal (vcdisk.vcbulge) galaxy components.\n\n\n\n\n\n\nimport vcdisk\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\n# suppressing warnings in the scipy.integrate.quad calls in vcbulge\nimport warnings\nfrom scipy.integrate import IntegrationWarning\nwarnings.filterwarnings(\"ignore\", category=IntegrationWarning)\n\n\n\n\nLet’s start with the case of a thick disk with a classical exponential surface density.\n\nmd, rd = 1e10, 2.0                        # mass, scalelength of the disk\nr = np.linspace(0.1, 30.0, 100)           # radii samples\n\ndef expdisk_sb(r, md, rd):\n    # exponential disk surface density\n    return md / (2*np.pi*rd**2) * np.exp(-r/rd)\n\nsb_exp = expdisk_sb(r, md, rd)\n\n# run vcdisk\nvdisk = vcdisk.vcdisk(r, sb_exp)\n\n\ndef plot_sb_vdisk(ax, r, sb, vdisk, label=None):\n    ax[0].plot(r, np.log10(sb), lw=2)\n    ax[0].set_xlabel(\"radius / kpc\");\n    ax[0].set_ylabel(\"log surface density / M_sun kpc\"+\"$^{2}$\");\n\n    ax[1].plot(r, vdisk, lw=2,  label=label)\n    ax[1].set_xlabel(\"radius / kpc\");\n    ax[1].set_ylabel(\"velocity / km s\"+\"$^{-1}$\");\n    if label is not None: ax[1].legend()\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\nplot_sb_vdisk(ax, r, sb_exp, vdisk)\n\n\n\n\nWe can explore how does \\(V_{\\rm disk}\\) change when changing the scaleheight of the disk \\(z_0\\) for instance:\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\nplot_sb_vdisk(ax, r, sb_exp, vdisk, label='z0=0.3 kpc')\nplot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, z0=0.1), label='z0=0.1 kpc')\nplot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, z0=0.8), label='z0=0.8 kpc')\n\n\n\n\n\n\n\nLet’s compare to other popular analytic surface density profiles. Probably the most used in Astronomy is the Sersic (1968) profile, which is often used to describe all sorts of stellar components, including disks and bulges. We can use the class sersic in the vcdisk package to get Sersic profiles\n\nsers_n1  = vcdisk.sersic(md, 1.678*rd, 1.0)\nsers_n2  = vcdisk.sersic(md, 1.678*rd, 2.0)\nsers_n05 = vcdisk.sersic(md, 1.678*rd, 0.5)\nsers_n4  = vcdisk.sersic(md, 1.678*rd, 4.0)\n\nHere, we have taken the reference model sers_n1 which is equivalent to the exponential disk above, since \\(R_e\\simeq 1.678 R_{\\rm d}\\) for \\(n=1\\) exponential profiles. Let’s now compare the circular velocities of disks with Sersic surface brightnesses with different index \\(n\\), while keeping the total luminosity fixed.\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\nplot_sb_vdisk(ax, r, sers_n1(r),  vcdisk.vcdisk(r, sers_n1(r)), label='Sersic: n=1')\nplot_sb_vdisk(ax, r, sers_n05(r), vcdisk.vcdisk(r, sers_n05(r)), label='Sersic: n=0.5')\nplot_sb_vdisk(ax, r, sers_n2(r),  vcdisk.vcdisk(r, sers_n2(r)), label='Sersic: n=2')\nplot_sb_vdisk(ax, r, sers_n4(r),  vcdisk.vcdisk(r, sers_n4(r)), label='Sersic: n=4')\nax[0].set_ylim(2,10);\nax[1].set_ylim(-1, 105);\n\n\n\n\n\n\n\nWe have just computed the circular velocities of thick galaxy disks whose surface brightness profile is of the Sersic form with different \\(n\\). However, we may also be interested in the circular velocity of a spheroidal component, like a stellar bulge, whose observed surface brightness is well approximated by a Sersic law.\n\n\nLet’s now compute the circular velocity on the mid-plane of a nearly spherical Sersic bulges with different \\(n\\).\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\nplot_sb_vdisk(ax, r, sers_n1(r),  vcdisk.vcbulge_sersic(r, md, 1.678*rd, 1.0), label='Sersic: n=1')\nplot_sb_vdisk(ax, r, sers_n05(r), vcdisk.vcbulge_sersic(r, md, 1.678*rd, 0.5), label='Sersic: n=0.5')\nplot_sb_vdisk(ax, r, sers_n2(r),  vcdisk.vcbulge_sersic(r, md, 1.678*rd, 2.0), label='Sersic: n=2')\nplot_sb_vdisk(ax, r, sers_n4(r),  vcdisk.vcbulge_sersic(r, md, 1.678*rd, 4.0), label='Sersic: n=4')\nax[0].set_ylim(2,10);\nax[1].set_ylim(-1, 105);\n\n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/vcdisk.py:925: RuntimeWarning: invalid value encountered in double_scalars\n  return -self.Ie*self.bn/(self.n*self.re) * np.exp(-self.bn*((R/self.re)**(1/self.n)-1)) * (R/self.re)**(1/self.n-1.0)\n\n\n\n\n\n\n\n\n\nWe can compute the circular velocity also for flattened oblate bulges, thus for components whose 3D density is stratified as \\(\\rho=\\rho(m)\\) where \\(m^2=R^2+z^2/q^2\\) with axis ratio \\(0<q\\leq 1\\).\nLet’s take the same baseline model as in Fig. 2 (top central panel) of Noordermeer (2008):\n\nmb, re = 5e9, 1.0                    # mass, effective radius of the bulge\nrb = np.logspace(-2, 1, 100)          # radii samples\n\nsers_bulge = vcdisk.sersic(mb, re, 1.0)\nsb_sers_bulge = sers_bulge(rb)\n\nand we can plot \\(V_{\\rm bulge}\\) for different axis ratios \\(q\\) at a fixed mass, effective radius and Sersic index. For comparison, the black dotted line is for a point mass with the same mass as the bulge.\n\n%%time\nplt.plot(rb, np.sqrt(4.301e-6*5e9/rb),'k:')\nplt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge), c='C0', label='q=0.99')\nplt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.8), c='C1', label='q=0.8')\nplt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.6), c='C2', label='q=0.6')\nplt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.4), c='C3', label='q=0.4')\nplt.plot(rb, vcdisk.vcbulge(rb, sb_sers_bulge, q=0.2), c='C4', label='q=0.2')\nplt.legend()\nplt.ylim(0,160);\nplt.xlim(0,10);\nplt.xlabel(\"radius / kpc\");\nplt.ylabel(\"velocity / km s\"+\"$^{-1}$\");\n\nCPU times: user 52 s, sys: 541 ms, total: 52.5 s\nWall time: 54.5 s\n\n\nText(0, 0.5, 'velocity / km s$^{-1}$')\n\n\n\n\n\n\n\n\nvcdisk allows us to specify our own vertical density profile, if we so choose. This can be easily done through the rhoz argument.\n\ndef rhoz_gauss(z, m, std): return np.exp(-0.5*(z-m)**2/std**2) / np.sqrt(2*np.pi*std**2)\n\ndef plot_rhoz(ax, z, rhoz, label=None):\n    ax.plot(z, rhoz, lw=2, label=label)\n    ax.legend()\n    ax.set_xlabel(\"radius / kpc\");\n    ax.set_ylabel(\"vertical density / M_sun kpc\"+\"$^{2}$\");\n\n\nfig, ax = plt.subplots(figsize=(6,4))\nz = np.linspace(0,3)\nplot_rhoz(ax, z, np.exp(-z/0.3) / (2*0.3), label='exp')\nplot_rhoz(ax, z, rhoz_gauss(z, 0., 0.8), label='gauss')\n\n\n\n\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\n\nplot_sb_vdisk(ax, r, sb_exp, vdisk)\nplot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, rhoz=rhoz_gauss, rhoz_args={'m':0.0, 'std':0.8}))\n\n\n\n\n\n\n\nWe can also have a flaring disk, i.e. one where the vertical density depends also on radius \\(\\rho_z=\\rho_z(z,R)\\). We can work out this case as well with vcdisk by specifying how \\(\\rho_z\\) depends on \\(R\\).\n\ndef rhoz_exp_flaring(z, R, z00, Rs):\n    z0R = z00+np.arcsinh(R**2/Rs**2)\n    return np.exp(-z/z0R) / (2*z0R)\n\nfig, ax = plt.subplots(figsize=(6,4))\nz = np.linspace(0,3)\nplot_rhoz(ax, z, rhoz_exp_flaring(z, 0.0, 0.1, 0.8), label='R=0')\nplot_rhoz(ax, z, rhoz_exp_flaring(z, 0.5, 0.1, 0.8), label='R=0.5')\nplot_rhoz(ax, z, rhoz_exp_flaring(z, 1.0, 0.1, 0.8), label='R=1')\nplot_rhoz(ax, z, rhoz_exp_flaring(z, 1.5, 0.1, 0.8), label='R=1.5')\nplt.ylim(None,2);\n\n\n\n\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\n\nplot_sb_vdisk(ax, r, sb_exp, vdisk)\nplot_sb_vdisk(ax, r, sb_exp, vcdisk.vcdisk(r, sb_exp, rhoz=rhoz_exp_flaring, rhoz_args={'z00':0.1, 'Rs':0.8}, flaring=True))\n\n\n\n\n\n\n\n\n\n\nLet’s say that, instead of having just simple galaxy images from which we can derive an analytic approximation for the galaxy surface brightness, we have a detailed galaxy image from which we can measure the intensity in elliptical annuli. This is for instance the case of nearby galaxies such as NGC 2403, that I use here as an example.\nI use data taken from the SPARC database for this galaxy, which I include in this package as the NGC2403_rotmod.dat file. Here are reported measurements of the intensity at 3.6\\(\\mu\\)m taken with the SPITZER space telescope in elliptical annuli for this target.\n\nrad, _, _, _, _, _, sb_n2403, _ = np.genfromtxt('NGC2403_rotmod.dat', unpack=True)\n# convert SB to Msun / kpc^2\nsb_n2403 *= 1e6\n\nNow we can very easily use vcdisk to calculate \\(V_{\\star, \\rm disk}\\) for this galaxy, and we can evena compare it to some analytic profiles.\n\nvd_n2403 = vcdisk.vcdisk(rad, sb_n2403, z0=0.4)# z0=0.4 kpc from Fraternali et al. (2002): https://ui.adsabs.harvard.edu/abs/2002AJ....123.3124F\nsb_exp_n2403 = expdisk_sb(rad, 1.004e10, 1.39) # numbers taken from http://astroweb.cwru.edu/SPARC/\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\n\nplot_sb_vdisk(ax, rad, sb_exp_n2403, vcdisk.vcdisk(rad, sb_exp_n2403), label='exp-disk')\nplot_sb_vdisk(ax, rad, sb_n2403, vd_n2403, label='observed')\n\n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n  \n\n\n\n\n\nThe \\(V_{\\star, \\rm disk}\\) curve obtained with vcdisk directly from the ellipse photometry of NGC 2403 is much more structured between \\(1-5\\rm \\,kpc\\), which is where the surface brightness has dips and peaks. Capturing this rich structure is essential to rotation curve modelling.\n\n\n\nLet’s take a different case now of an early spiral galaxy, i.e. a disk galaxy with redder colors and a prominent central bulge: UGC 2953.\nI use again the data taken from the SPARC database and I include them here as the UGC02953_rotmod.dat file. There are measurements of the near-infrared intensities of both the disk and bulge components separated.\n\nrad, _, _, _, _, _, sbd_u2953, sbb_u2953 = np.genfromtxt('UGC02953_rotmod.dat', unpack=True)\n# convert SB to Msun / kpc^2\nsbd_u2953 *= 1e6\nsbb_u2953 *= 1e6\n\nWith the two different surface brightness arrays, we can use vcdisk and vcbulge to get the corresponding circular velocities.\n\nvd_u2953 = vcdisk.vcdisk(rad, sbd_u2953, z0=0.6)\nvb_u2953 = vcdisk.vcbulge(rad, sbb_u2953)\n\nAnd we can finally plot them together.\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\n\nplot_sb_vdisk(ax, rad, sbd_u2953, vd_u2953, label='disk')\nplot_sb_vdisk(ax, rad, sbb_u2953, vb_u2953, label='bulge')\nplot_sb_vdisk(ax, rad, sbd_u2953+sbb_u2953, np.sqrt(vd_u2953**2+vb_u2953**2), label='total')\n\n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n  \n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n  \n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10"
  },
  {
    "objectID": "posts/2022-11-02-gp_rotcurves.html",
    "href": "posts/2022-11-02-gp_rotcurves.html",
    "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
    "section": "",
    "text": "#hide Copyright (c) 2022, Lorenzo Posti All rights reserved.\nThis source code is licensed under the BSD-style license found in the license_notebooks file in the root directory of this source tree."
  },
  {
    "objectID": "posts/2022-11-02-gp_rotcurves.html#attribution-license",
    "href": "posts/2022-11-02-gp_rotcurves.html#attribution-license",
    "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
    "section": "Attribution & License",
    "text": "Attribution & License\nIf you use parts of the code found in this notebook please cite the paper Posti (2022), Res. Notes AAS, 6, 233.\nCopyright (c) 2022, Lorenzo Posti. The code is distributed under BSD-style license and it can be copied and used."
  },
  {
    "objectID": "posts/2022-11-02-gp_rotcurves.html#introduction",
    "href": "posts/2022-11-02-gp_rotcurves.html#introduction",
    "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
    "section": "Introduction",
    "text": "Introduction\nGalaxy rotation curves are usually modelled by assuming that each datapoint in the curve is independent from the others. However, this is naturally just a first order approximation, since observational effects such due to geometrical projection and resolution, as well as physical effects such as non-circular motions, can make the velocities measured in two adjacent annnuli significantly correlated.\nIn this notebook I use the rotation curve of NGC 2403 as a test case to show how to include Gaussian Processes (GPs) in rotation curve decomposition models, in order to account for underlying data correlations. More details can be found in the accompanying paper Posti (2022), Res. Notes AAS, 6, 233.\n\n\nCode\nimport numpy as np\nimport matplotlib\nimport matplotlib.pylab as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport random\nimport jax\nimport jax.numpy as jnp\nimport jaxopt\nfrom functools import partial\nfrom tinygp import GaussianProcess, kernels\nimport numpyro\nimport arviz\nimport corner\n\njax.config.update(\"jax_enable_x64\", True)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom matplotlib import rc\nrc('text', usetex=True)\n\n%config Completer.use_jedi = False\n%matplotlib inline"
  },
  {
    "objectID": "posts/2022-11-02-gp_rotcurves.html#data",
    "href": "posts/2022-11-02-gp_rotcurves.html#data",
    "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
    "section": "Data",
    "text": "Data\nHere I introduce the data for the galaxy NGC 2403 and the functions needed to work with Eq. (1) in the paper.\nNote that the code below works also for any other galaxy whose data are formatted in the same way, e.g. it works with no modifications needed for all galaxies in the SPARC catalog (Lelli et al. 2016).\n\nDefinitions of functions for curve decomposition\nI start with some definitions to specify \\(V_{\\rm DM}(R)\\), the contribution of DM to the circular velocity in Eq. (1). I assume NFW profiles for the DM halos, which are specified by two parameters: halo mass \\(M_{\\rm h}\\) and concentration \\(c\\). \\(M_{\\rm h}\\) is the virial mass defined with a critical overdensity of \\(\\Delta_{\\rm c}=200\\).\n\nG = 4.301e-9 # gravitational constant, in Mpc km^2 s^-2 Msun^-1\nH = 70.      # Hubble's constant, in km s^-1 Mpc^-1\nDc= 200.     # critical overdensity\n\nNote that below I use jax, and not numpy, to define these functions. This is needed in order to do model inference with numpyro.\n\n# accessory function for NFW halos\ndef jax_fc(x): return jnp.log(1+x)-x/(1+x)\n\n# definitions of virial velocity and virial radius\ndef jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.))\ndef jax_Rvir(Mh):\n    rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G))\n    return 1e3 * ((Mh / rho_hat)**(1./3.))\n\n# V_DM(R) for an NFW halo\ndef jax_vhalo(params, R):\n    Mh, cc = 10**params['log_mh'], 10**params['log_c']    \n    rv = jax_Rvir(Mh)\n    return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) \n\nFinally, below I implement the model rotation curve in Eq. (1). Two things are worth pointing out: - I use a linear interpolation for the baryonic part of the curve, since I assume that \\(V_{\\rm gas}\\) and \\(V_\\star\\) are measured at fixed radii, so that params['r'], params['vg'] etc. are expected to be arrays of the same size. - I decomposed \\(V_\\star\\) in the bulge and disk components, with two different mass-to-light ratios. While my test case NGC 2403 has no bulge, it is good to include it here for the sake of generality. The SPARC catalog indeed includes stellar circular velocities decomposed into bulge and disk.\n\ndef jax_vmod(params, R):\n    return jnp.sqrt(jax_vhalo(params, R)**2 + # DM\n                    jnp.interp(R, params['r'], params['vg']**2 + # gas\n                               10**params['log_mld']*params['vd']**2+10**params['log_mlb']*params['vb']**2 # stars\n                              ))\n\n\n\nRotation curve data for NGC 2403\nI take the rotation curve data of NGC 2403 from the SPARC catalog at face value. I include a copy of the file here for convenience.\n\nr, vobs, e_vobs, vg, vd, vb, _, _ = np.genfromtxt('data/NGC2403_rotmod.dat', unpack=True)\n\nLet’s plot the data below together with the best-fit model obtained by Posti et al. (2019) for reference.\n\nparams = {\n    # parameters of the best-fit in Posti et al. (2019)\n    'log_mh' : 11.4,\n    'log_c'  : 1.14,\n    'log_mld': -0.377,\n    'log_mlb': -99., # this galaxy has no bulge\n    # data arrrays\n    'r'  : r,\n    'vg' : vg,\n    'vd' : vd,\n    'vb' : vb,\n}\n\nfig,ax = plt.subplots(figsize=(8,5))\nax.errorbar(r, vobs, yerr=e_vobs, fmt='.', c='k', lw=0.5, label=r'$\\rm data$')\nax.plot(r, vg, ':', c='tab:cyan', label=r'$\\rm gas$')\nax.plot(r, np.sqrt(10**params['log_mld']*vd**2+10**params['log_mlb']*vb**2), '--', c='tab:olive', label=r'$\\rm stars$')\nax.plot(r, jax_vhalo(params, r), '-.', c='tab:purple', label=r'$\\rm DM$')\nax.plot(r, jax_vmod(params, r), c='grey', lw=2, label=r'$\\rm fit$')\nax.set_xlabel(r\"$\\rm radius/kpc$\", fontsize=18)\nax.set_ylabel(r\"$\\rm velocity/km\\,s^{-1}$\", fontsize=18)\nax.set_title(r\"$\\rm NGC 2403$\", fontsize=20);\nax.legend(loc='lower right', fontsize=14);\nax.tick_params(labelsize=14);"
  },
  {
    "objectID": "posts/2022-11-02-gp_rotcurves.html#models-with-or-without-gps",
    "href": "posts/2022-11-02-gp_rotcurves.html#models-with-or-without-gps",
    "title": "Rotation curve decompositions with Gaussian Processes: taking into account data correlations leads to unbiased results",
    "section": "Models with or without GPs",
    "text": "Models with or without GPs\nLet’s now get to the modelling side of things. I set up two models here. The first one is analogous to Posti et al. (2019), as well as many other works in this context (e.g. Katz et al. 2017, Li et al. 2020, Mancera-Pina et al. 2022, di Teodoro et al. 2022), it has a \\(\\chi^2\\) likelihood and it implicitly assumes that the rotation curve datapoints are independent. The second one generalizes this model by using GPs to take into account data correlations. I recommend the recent review by Aigrain & Foreman-Mackey (2022), in particular their first section, as an introduction to GPs.\nI use the library tinygp to set up my GP regression problem and I use numpyro to sample the posterior distribution. In particular, I use and Hamiltonian Monte Carlo sampler called No U-Turn Sampler (NUTS).\n\nGaussian Processes\nLet’s generate GPs with an Exp-Squared kernel with two parameters, an amplitude \\(A_k\\) and a scale \\(s_k\\), i.e. \\[\nk(R_i, R_j) = A_k \\exp\\left[-\\frac{1}{2}\\left(\\frac{|R_i-R_j|}{s_k}\\right)^2\\right]\n\\] This kernel is said to be stationary because it depends only on the distance between two points \\(|R_i-R_j|\\).\n\ndef build_gp(params, x, yerr):\n    kernel = 10**params['log_amp']*kernels.ExpSquared(10**params['log_scl'], distance=kernels.distance.L1Distance())\n    return GaussianProcess(kernel, \n                           x, \n                           diag=yerr**2, \n                           mean=partial(jax_vmod, params)\n                          )\n\n\n\nnumpyro models\nI now set up the model’s posterior distribution to be sampled by numpyro, thus it is a function containing pyro primitives.\nThe model starts by defining the priors on the physical parameters (\\(\\theta_V\\) in the paper). I use: - uninformative prior on \\(\\log\\,M_{\\rm h}\\) - Gaussian on \\(\\log\\,c\\), with mean and width following the \\(c-M_{\\rm h}\\) relation found in cosmological simulations (Dutton & Maccio’ 2014) - Gaussian on \\(\\log\\,(M/L)_{\\rm D}\\), centred on \\((M/L)_{\\rm D}=0.5\\) and with standard deviation of 0.2 dex (compatible with stellar population synthesis models, e.g. Lelli et al. 2016) - Gaussian on \\(\\log\\,(M/L)_{\\rm B}\\), centred on \\((M/L)_{\\rm B}=0.7\\) and with standard deviation of 0.2 dex (again, see Lelli et al. 2016). Note that this is not used in the case of NGC 2403\nAfter the definition of the priors the function branches out: one branch with GPs and one without. I borrowed this structure from the transit example of Fig. 3 in Aigrain & Foreman-Mackey (2022).\nThe branch with GP also implement additional priors for the two parameters of the kernel (\\(\\theta_k\\) in the paper), i.e. the amplitude and scale.\n\ndef model(t, y_err, y, params, use_gp=False):\n    \n    # priors\n    params[\"log_mh\"]=numpyro.sample(\"log_mh\",numpyro.distributions.Uniform(8.0,  14.0)) \n    params[\"log_c\"] =numpyro.sample('log_c',numpyro.distributions.Normal(0.905-0.101*(params['log_mh']*0.7-12),0.11)) \n    params[\"log_mld\"]=numpyro.sample('log_mld',numpyro.distributions.Normal(-0.3, 0.2)) \n    params[\"log_mlb\"]=numpyro.sample('log_mlb',numpyro.distributions.Normal(-0.15, 0.2)) \n    \n    if use_gp:\n        ###################\n        # branch WITH GPs #\n        ###################\n        \n        # define kernel parameters\n        params[\"log_amp\"] = numpyro.sample(\"log_amp\", numpyro.distributions.Uniform(-4.0, 5.0))\n        params[\"log_scl\"] = numpyro.sample(\"log_scl\", numpyro.distributions.Uniform(-2.0, 3.0))\n        \n        # generate the GP\n        gp = build_gp(params, t, y_err)\n        \n        # sample the posterior\n        numpyro.sample(\"y\", gp.numpyro_dist(), obs=y)\n        \n        # calculate the predicted V_rot (i.e. the mean function) of the model\n        mu = gp.mean_function(params[\"r_grid\"])\n        numpyro.deterministic(\"mu\", mu)\n\n    else:\n        ######################\n        # branch WITHOUT GPs #\n        ######################\n        \n        # sample the posterior\n        numpyro.sample(\"y\", numpyro.distributions.Normal(jax_vmod(params, t), y_err), obs=y)\n        \n        # calculate properties of the model\n        numpyro.deterministic(\"mu\", jax_vmod(params, params[\"r_grid\"]))\n\n\n\nRunning the model without GP, i.e. assuming independent datapoints\nI start by sampling the posterior of the model akin to that of Posti et al. (2019), i.e. assuming that the points in the curve are independent.\nI’m using arviz to analyse the posterior sampled by NUTS. In particular, keep an eye on r_hat which is the Gelman-Rubin statistics: for our purposes, we can use \\({\\rm r_{hat}}\\simeq 1\\) as an indicator that the marginalized posterior on a particular parameter is well determined.\n\ngrid_size = 1000\nr_grid = jnp.linspace(r.min(), r.max(), grid_size) # radial grid on which to predict V_rot(R)\nparams = {\"vg\"    : vg,\n          \"vd\"    : vd,\n          \"vb\"    : vb,\n          \"r\"     : r,\n          \"r_grid\": r_grid}\n\n\nnum_warmup=1000\nnum_samples=3000\nnum_chains=3\naccept_prob = 0.9\n\nsampler_wn = numpyro.infer.MCMC(\n    numpyro.infer.NUTS(\n        model,\n        dense_mass=True,\n        target_accept_prob=accept_prob,\n    ),\n    num_warmup=num_warmup,\n    num_samples=num_samples,\n    num_chains=num_chains,\n    progress_bar=True,\n)\n%time sampler_wn.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params)\n\ninf_data_wn = arviz.from_numpyro(sampler_wn)\narviz.summary(inf_data_wn, var_names=[\"log_mh\", \"log_c\", \"log_mld\"])\n\nsample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14<00:00, 277.33it/s, 15 steps of size 2.45e-01. acc. prob=0.92]\nsample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14<00:00, 281.80it/s, 15 steps of size 2.07e-01. acc. prob=0.95]\nsample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:13<00:00, 288.30it/s, 15 steps of size 1.95e-01. acc. prob=0.95]\n\n\nCPU times: user 43.9 s, sys: 807 ms, total: 44.7 s\nWall time: 45.7 s\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      log_mh\n      11.325\n      0.014\n      11.298\n      11.351\n      0.000\n      0.000\n      3623.0\n      4045.0\n      1.0\n    \n    \n      log_c\n      1.234\n      0.019\n      1.198\n      1.268\n      0.000\n      0.000\n      3382.0\n      3801.0\n      1.0\n    \n    \n      log_mld\n      -0.542\n      0.045\n      -0.632\n      -0.463\n      0.001\n      0.001\n      3267.0\n      3282.0\n      1.0\n    \n  \n\n\n\n\nLet’s plot the chains to make sure each parameter is converged\n\nazLabeller = arviz.labels.MapLabeller(var_name_map={\"log_mh\" : r\"$\\log\\,M_{\\rm h}$\",\n                                                    \"log_c\"  : r\"$\\log\\,c$\", \n                                                    \"log_mld\": r\"$\\log\\,M/L$\",\n                                                    \"log_amp\": r\"$A_k$\",\n                                                    \"log_scl\": r\"$s_k$\"})\narviz.plot_trace(inf_data_wn, var_names=[\"log_mh\", \"log_c\", \"log_mld\"], figsize=(12,9), \n                 labeller = azLabeller);\n\n\n\n\n\n\nRunning the model with GP, i.e. taking into account data correlations\nNow, let’s have a look at what happens when GPs come into play. I run exactly the same procedure as before to sample the model’s posterior, but this time I select the branch with use_gp=True. Since now I have two more free parameters, the scale and amplitude of the kernel, I also initialize these two in init_strategy.\n\nsampler = numpyro.infer.MCMC(\n    numpyro.infer.NUTS(\n        model,\n        dense_mass=True,\n        target_accept_prob=accept_prob,\n    ),\n    num_warmup=num_warmup,\n    num_samples=num_samples,\n    num_chains=num_chains,\n    progress_bar=True,\n)\n%time sampler.run(jax.random.PRNGKey(11), r, e_vobs, vobs, params, use_gp=True)\n\ninf_data = arviz.from_numpyro(sampler)\narviz.summary(inf_data, var_names=[\"log_mh\", \"log_c\", \"log_mld\", \"log_amp\", \"log_scl\"])\n\nsample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:29<00:00, 136.37it/s, 15 steps of size 3.46e-01. acc. prob=0.96]\nsample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 146.69it/s, 7 steps of size 3.67e-01. acc. prob=0.96]\nsample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:25<00:00, 159.04it/s, 7 steps of size 4.01e-01. acc. prob=0.94]\n\n\nCPU times: user 2min 27s, sys: 2.85 s, total: 2min 30s\nWall time: 1min 38s\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      log_mh\n      11.491\n      0.076\n      11.353\n      11.642\n      0.001\n      0.001\n      5466.0\n      4897.0\n      1.0\n    \n    \n      log_c\n      1.079\n      0.065\n      0.956\n      1.201\n      0.001\n      0.001\n      5602.0\n      4905.0\n      1.0\n    \n    \n      log_mld\n      -0.326\n      0.075\n      -0.470\n      -0.191\n      0.001\n      0.001\n      5531.0\n      4435.0\n      1.0\n    \n    \n      log_amp\n      1.104\n      0.270\n      0.624\n      1.602\n      0.005\n      0.004\n      3793.0\n      3277.0\n      1.0\n    \n    \n      log_scl\n      -0.018\n      0.087\n      -0.188\n      0.131\n      0.001\n      0.001\n      3788.0\n      3652.0\n      1.0\n    \n  \n\n\n\n\nAnd let’s plot again the chains to make sure that every parameter has converged\n\narviz.plot_trace(inf_data, var_names=[\"log_mh\", \"log_c\", \"log_mld\", \"log_amp\", \"log_scl\"], figsize=(12,15), \n                 labeller = azLabeller);\n\n\n\n\nEverything looks good for both models.\n\n\nComparing the predictions of the two models\nFinally, I can compare the posterior distributions of the two models. I use corner to plot 1-D and 2-D projections of the posteriors.\n\nranges = [(11.2, 11.8), (0.8, 1.35), (-0.8,-0.0)] # NGC 2403\n\nfig = corner.corner(inf_data_wn, bins=40, range=ranges, \n                    color=\"C0\", var_names=[\"log_mh\", \"log_c\", \"log_mld\"], smooth=1.0, smooth1d=1.0\n                   )\nfig = corner.corner(inf_data, bins=40, range=ranges, \n                    color=\"C1\", var_names=[\"log_mh\", \"log_c\", \"log_mld\"], smooth=1.0, smooth1d=1.0,\n                    labels=[\"$\\log\\,M_h$\", \"$\\log\\,c$\", r\"$\\log\\,M/L$\"],\n                    label_kwargs={\"fontsize\":20}, fig=fig)\n\n\n# make legend\nax = fig.axes[1]\nkey_nn = matplotlib.lines.Line2D([], [], color='C0',    linestyle='-', \n                                 label=r'$\\rm Model\\,\\,{\\bf without\\,\\,GPs}:\\,\\,independent\\,\\,data$')\nkey_gp = matplotlib.lines.Line2D([], [], color='C1',   linestyle='-', \n                                 label=r'$\\rm Model\\,\\,{\\bf with\\,\\,GPs}:\\,\\,correlated\\,\\,data$')\nax.legend(loc='upper left', handles=[key_nn, key_gp], fontsize=16);\n\n\n\n\nThen I can compare the predicted curve decompositions of the two models. Here I get the predictions of the model, excluding the warmup phase during sampling.\n\nfinal_shape = (num_chains*(num_samples-num_warmup),grid_size) # shape of the predictions array after removing warmup\n\npred_wn = sampler_wn.get_samples(group_by_chain=True)['mu'][:,num_warmup:,:].reshape(final_shape)\npred    = sampler.get_samples(group_by_chain=True)['mu'][:,num_warmup:,:].reshape(final_shape)\n\n\nfig,ax = plt.subplots(figsize=(7,8), nrows=2, gridspec_kw={'hspace':0})\n\ndef commons(ax, i, lmh, lc, lmld, lmlb, leg=True):\n    # rotation curve\n    ax[i].errorbar(r, vobs, yerr=e_vobs, fmt='.', c='k', lw=0.5, label=r'$\\rm data$', markersize=4)\n    ax[i].plot(r, vg, ':', c='tab:cyan', label=r'$\\rm gas$')\n    ax[i].plot(r, np.sqrt(10**lmld*vd**2+10**lmlb*vb**2), '--', c='tab:olive', label=r'$\\rm stars$')\n    ax[i].plot(r, jax_vhalo({'log_mh':lmh, 'log_c':lc}, r), '-.', c='tab:purple', label=r'$\\rm DM$')\n    if leg: ax[i].legend(loc='lower right', fontsize=14)\n    ax[i].set_xlabel(r\"$\\rm radius/kpc$\", fontsize=18)\n    ax[i].set_ylabel(r\"$\\rm velocity/km\\,s^{-1}$\", fontsize=18)\n    ax[i].tick_params(direction='inout', top=True, labelsize=14)\n    \n    \nax[0].fill_between(r_grid, np.percentile(pred_wn,2.1,axis=0), np.percentile(pred_wn,97.9,axis=0), \n                     facecolor='C0', alpha=0.3)\nax[1].fill_between(r_grid, np.percentile(pred,2.1,axis=0), np.percentile(pred,97.9,axis=0), \n                     facecolor='C1', alpha=0.3)\nax[0].plot(r_grid, np.median(pred_wn, axis=0), 'C0')\nax[1].plot(r_grid, np.median(pred, axis=0), 'C1')\ncommons(ax, 0, 11.325, 1.234, -0.542, -99.)\ncommons(ax, 1, 11.491, 1.079, -0.326, -99., leg=False)\n\n\n\n\n\n\nCorrelation matrix\nGiven that the parameters of the kernel \\(A_k\\) and \\(s_k\\) are well constrained by the model, we can have a look at the correlation matrix of the model with GPs.\n\n%%time\ndef get_kmat(params, x, yerr,):\n    gp = build_gp(params, x, yerr)\n\n    xm1, xm2 = jnp.meshgrid(x,x)\n    zm = np.zeros_like(xm1.flatten())\n    for i in range(len(xm1.flatten())):\n        zm[i]=gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i])\n\n    return zm.reshape((len(x), len(x)))\n\nzm = get_kmat({\"log_amp\":1.104, \"log_scl\":-0.018, 'log_mh':11.325, 'log_c':1.234, \n               'log_mld':-0.542, 'log_mlb':-99., 'vg':vg, 'vd':vd, 'vb':vb, 'r':r}, r, e_vobs)\n\nCPU times: user 1min 35s, sys: 2.17 s, total: 1min 37s\nWall time: 2min\n\n\n\ndef plt_mat(ax, zm):\n    \n    im=ax.matshow(zm)\n    ax.set_xlabel(\"$i$\", fontsize=20)\n    ax.set_ylabel(\"$j$\", fontsize=20)\n    ax.tick_params(labelsize=14)\n    ax.xaxis.set_label_position('top') \n    \n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n\n    cb = plt.colorbar(im, cax=cax)\n    cb.set_label(r\"$k(R_i, R_j)$\", fontsize=22)\n    cb.ax.tick_params(labelsize=16)\n\n\nfig,ax = plt.subplots()\nplt_mat(ax, zm)\n\n\n\n\nThe peculiar 2-block shape of this matrix is due to the combined H\\(\\alpha\\)-HI nature of the rotation curve. The optical H\\(\\alpha\\) curve samples the inner regions of the galaxy, up to \\(\\sim 5\\) kpc, with a finer spatial sampling of 0.1 kpc with respect to the HI rotation curve, which instead has a spacing of 0.5 kpc and is from 5 kpc outwards."
  },
  {
    "objectID": "posts/2022-06-10-autoencoder-rotcurves.html",
    "href": "posts/2022-06-10-autoencoder-rotcurves.html",
    "title": "Autoencoder represents a multi-dimensional smooth function",
    "section": "",
    "text": "In this piece I’m interested in a neural network that is not designed to predict an outcome given a dataset, but rather in an algorithm that is capable of learning an underlying - more compressed - representation of the dataset at hand. This can be used for a number of excitng applications such as data compression, latent representation or recovering the true data distribution. An autoencoder, which is a special encoder/decoder network, is an efficient algorithm that can achieve this. It is divided in two parts: - encoder: the algorithm learns a simple (lower dimensional) representation of the data (code) - decoder: starting from an instance of such representation, code, the algorithm develops it returning the data in the orignal (higher dimensional) form.\nThe Autoencoder is then a map $ x x$, where the first part is the encoder and the second is the decoder; thus it is effectively a map of \\(x\\) onto itself. This implies 1) that we can use the data itself \\(x\\) in the loss function and 2) that the algorithm is learning a representation of the dataset itself.\n\n\nCode\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.special import i0, i1, k0, k1\nfrom torch import tensor\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch, math\nimport random\n\n%config Completer.use_jedi = False\n%matplotlib inline\n\nrng = np.random.default_rng()\n\n\n\n\nTo test an autoencoder network we’ll use a simple rotation curve model that is the superposition of two massive components, a disc and an halo. Both components have circular velocity curves that depend on two parameters, a mass and a physical scale, such that the total model has 4 parameters, 2 for each component.\nThis is a nice case to test how an autoencoder learns an underlying lower dimensional representation of a dataset, since each rotation curve that we will supply to it is actually derived sampling a smooth function of just 4 parameters.\n\n\nCode\nG, H, Dc = 4.301e-9, 70, 200.\n\ndef fc(x):\n    return np.log(1+x)-x/(1+x)\n\ndef Vvir(Mh):\n    return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.))\n\ndef Rvir(Mh):\n    rho_c = 3. * (H)**2 / (8. * np.pi * G)\n    rho_hat = 4. / 3. * np.pi * Dc * rho_c\n    return 1e3 * np.power(Mh / rho_hat, 1./3.)\n\n\n\n# halo concentration--mass relation\ndef c(Mh, w_scatter=False, H=70.): \n    if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh)))\n    return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12))\n\n# disc mass--size relation\ndef getRd_fromMd(Md, w_scatter=False):\n    ''' approximate mass-size relation '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.3+0.5)\n\n# disc mass--halo mass relation\ndef getMh_fromMd(Md, w_scatter=False):\n    ''' approximate SHMR '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.75+12.0)\n\nSampling uniformly in disc mass, between \\(10^9\\) and \\(10^{12}\\) solar masses, and generating random samples of disc size, halo mass, and halo concentration following the above scaling relations.\n\nnsamp = 1000\nms = 10**rng.uniform(9, 12, nsamp)\nrd = getRd_fromMd(ms, w_scatter=True)\nmh = getMh_fromMd(ms, w_scatter=True)\ncc = c(mh, w_scatter=True)\n\nAbove we have generated our latent representation of the dataset, that is each galaxy model is represented by a quadruple (ms,rd,mh,cc). Below we construct a class to generate the rotation curve of the corresponding galaxy model.\n\nclass curveMod():\n    def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)):\n        self.G, self.H, self.Dc = 4.301e-9, 70, 200.  # physical constants\n        self.Md, self.Rd = Md, Rd\n        self.Mh, self.cc = Mh, cc\n        self.rad = rad\n        \n        if hasattr(self.Md, '__len__'):\n            self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))]\n            self.vdm   = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))]\n            self.vc    = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))]\n        else:\n            self.vdisc = self._vdisc(self.rad, self.Md, self.Rd)\n            self.vdm   = self._vhalo(self.rad, self.Mh, self.cc)\n            self.vc    = np.sqrt(self.vdisc**2+self.vdm**2)\n        \n    def _fc(self, x): return np.log(1+x)-x/(1+x)\n    def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.))\n    def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.)\n    \n    def _vhalo(self, R, Mh, cc):\n        # circular velocity of the halo component (NFW model)\n        rv = self._Rvir(Mh)\n        return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) \n    \n    def _vdisc(self, R, Md, Rd):\n        # circular velocity of the disc component (exponential disc)\n        y = R/2./Rd\n        return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y))))\n\nWe can now initialize this class with the samples (ms,rd,mh,cc) above, which will store inside the class the rotation curves of all the models - defined on the same radial scale (np.logscale(-1, np.log10(50), 50)) by default.\n\ncm=curveMod(ms,rd,mh,cc)\n\nLet’s plot all the rotation curves together:\n\nfor v in cm.vc: plt.plot(cm.rad, v)\nplt.xlabel('radius')\nplt.ylabel('velocity');\n\n\n\n\n\n\n\nWe can now build our Autoncoder class deriving from nn.Module. In this example, each rotation curve is a collection of 50 values (see the radial grid in curveMod) and we want to compress it down to a code of just 4 numbers. Notice that we start from the simplified case in which we assume to already know that the ideal latent representation has 4 parameters.\nThe encoder network has 3 layers, going from the initial \\(n=50\\) to \\(32\\), then to \\(16\\), and finally to \\(4\\). The decoder is symmetric, going from \\(n=4\\) to \\(16\\), then to \\(32\\), and finally to \\(50\\).\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, ninp, **kwargs):\n        super().__init__()\n        self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32)\n        self.encodeLayer2 = nn.Linear(in_features=32,   out_features=16)\n        self.encodeOut    = nn.Linear(in_features=16,   out_features=4)\n        self.decodeLayer1 = nn.Linear(in_features=4,    out_features=16)\n        self.decodeLayer2 = nn.Linear(in_features=16,   out_features=32)\n        self.decodeOut    = nn.Linear(in_features=32,   out_features=ninp)\n        \n    def encoder(self, x):       return self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x)))))\n    def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded)))))\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\nAutoEncoder(len(cm.rad))\n\nAutoEncoder(\n  (encodeLayer1): Linear(in_features=50, out_features=32, bias=True)\n  (encodeLayer2): Linear(in_features=32, out_features=16, bias=True)\n  (encodeOut): Linear(in_features=16, out_features=4, bias=True)\n  (decodeLayer1): Linear(in_features=4, out_features=16, bias=True)\n  (decodeLayer2): Linear(in_features=16, out_features=32, bias=True)\n  (decodeOut): Linear(in_features=32, out_features=50, bias=True)\n)\n\n\n\n\n\nWe now shuffle, normalize, and split the rotation curve dataset into training and validation with a 20% validation split.\n\ndef datanorm(x):  return (x-x.mean())/x.std(), x.mean(), x.std()\ndef datascale(x, m, s): return x*s+m\n\nidshuff = torch.randperm(nsamp)\nxdata = tensor(cm.vc, dtype=torch.float)[idshuff,:]\nxdata, xmean, xstd = datanorm(xdata)\n\nfval = 0.20\nxtrain = xdata[:int(nsamp*(1.0-fval))]\nxvalid = xdata[int(nsamp*(1.0-fval)):]\n\n\n\n\nWe now initialize the training loop, defining a simple MSE loss function and a standard Adam optimizer, and we start training\n\nae = AutoEncoder(len(cm.rad))\n\n# Adam and MSE Loss\nloss_func = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(ae.parameters(), lr=0.01)\n\nfor epoch in range(1001):\n    ymod = ae.forward(xtrain)\n    loss = loss_func(xtrain, ymod)\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    optimizer.zero_grad()\n    \n    if epoch%50==0: print (epoch, \"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % loss_func(xvalid, ae.forward(xvalid)))\n\n0 train L:1.04e+00   valid L:9.36e-01\n50 train L:2.77e-02   valid L:2.33e-02\n100 train L:5.27e-03   valid L:5.16e-03\n150 train L:3.13e-03   valid L:3.33e-03\n200 train L:2.59e-03   valid L:2.80e-03\n250 train L:2.26e-03   valid L:2.12e-03\n300 train L:1.59e-03   valid L:1.63e-03\n350 train L:1.04e-03   valid L:1.14e-03\n400 train L:9.29e-04   valid L:9.83e-04\n450 train L:7.87e-04   valid L:8.51e-04\n500 train L:9.03e-04   valid L:1.10e-03\n550 train L:6.68e-04   valid L:7.73e-04\n600 train L:6.68e-04   valid L:7.54e-04\n650 train L:7.83e-04   valid L:7.54e-04\n700 train L:6.77e-04   valid L:7.19e-04\n750 train L:1.35e-03   valid L:1.92e-03\n800 train L:4.56e-04   valid L:5.63e-04\n850 train L:4.23e-04   valid L:5.46e-04\n900 train L:8.74e-04   valid L:1.07e-03\n950 train L:3.84e-04   valid L:4.79e-04\n1000 train L:4.45e-04   valid L:4.95e-04\n\n\nAfter 1000 epochs of trainig we get down to a low and stable MSE on the validation set. We can now compare the actual rotation curves in the validation set with those decoded by the model, finding an impressively good match!\n\nfig,ax = plt.subplots(figsize=(12,4), ncols=2)\nfor v in datascale(xvalid,xmean,xstd): ax[0].plot(cm.rad, v)\nfor v in datascale(ae.forward(xvalid),xmean,xstd): ax[1].plot(cm.rad, v.detach().numpy())\nax[0].set_xlabel('radius'); ax[1].set_xlabel('radius')\nax[0].set_ylabel('velocity');\n\n\n\n\n\n\n\nFinally we can explore a bit the properties of the 4 values encoded by the autoencoder and try to understand how do they relate to the original 4 physical parameters. Initially we generated each rotation curve starting from a 4-ple in (ms, mh, rd, cc), where these numbers are generated from some well known scaling relations. We plot the distribution of the initial 4 parameters here:\n\nfig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4)\n\npp = [ms, mh, rd, cc]\n\nfor i in range(4):\n    for j in range(4):\n        if j<i:  ax[i,j].scatter(np.log10(pp[j]), np.log10(pp[i]), s=2)\n        if j==i: ax[i,j].hist(np.log10(pp[i]), bins=15, lw=2, histtype='step');\n        if j>i:  ax[i,j].set_axis_off()\nax[3,0].set_xlabel('log10(ms)'); \nax[3,1].set_xlabel('log10(mh)'); ax[1,0].set_ylabel('log10(mh)');\nax[3,2].set_xlabel('log10(rd)'); ax[2,0].set_ylabel('log10(rd)');\nax[3,3].set_xlabel('log10(cc)'); ax[3,0].set_ylabel('log10(cc)');\n\n\n\n\nNow we can ask ourselves if similar correlations are observed in the 4 parameters coded by the autoencoder. In principle these are some other 4 numbers that fully specify a single rotation curve model, but that do not necessarily have anything to do with the original (ms, mh, rd, cc).\n\nfig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4)\n\npp_ae = ae.encoder(xtrain).detach()\n\nfor i in range(4):\n    for j in range(4):\n        if j<i:  ax[i,j].scatter(pp_ae[:,j], pp_ae[:,i], s=2)\n        if j==i: ax[i,j].hist(pp_ae[:,i].detach().numpy(), bins=15, lw=2, histtype='step');\n        if j>i:  ax[i,j].set_axis_off()\n\n\n\n\nWe can see that the coded parameters are indeed strongly correlated among themselves, however it is difficult to draw parallelisms with the behaviour we see in the original 4 parameters. An important difference to notice in these plots is that here we do not use a logarithmic scale for the 4 coded parameters, since they also take negative values unlike (ms, mh, rd, cc).\nLet’s now have a look at how the 4 coded parameters are correlated with the original ones. This is interesting since while the 4-dimensional latent space found by the autoencoder is not necessarily the original space of (ms, mh, rd, cc), the 4 new parameters might be well correlated with the 4 original physical quantites.\n\n\nCode\nmdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff]\nrdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff]\n\nith = int(nsamp*(1.0-fval))\nmdtrain, mhtrain = mdshuff[:ith], mhshuff[:ith]\nrdtrain, cctrain = rdshuff[:ith], ccshuff[:ith]\nmdvalid, mhvalid = mdshuff[ith:], mhshuff[ith:]\nrdvalid, ccvalid = rdshuff[ith:], ccshuff[ith:]\n\npartrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T)\nparvalid = (np.vstack([mdvalid, mhvalid, rdvalid, ccvalid]).T)\n\n\nPlotting the mutual correalations in the training set we do see that the 4 coded parameters are not at all randomly related to the original physical quantities.\n\nfig,ax = plt.subplots(figsize=(8,8), ncols=4, nrows=4)\n\npp_ae = ae.encoder(xtrain).detach()\n\nfor i in range(4):\n    for j in range(4):\n        if j<=i: ax[i,j].scatter(np.log10(partrain[:,j]), pp_ae[:,i], s=2)\n        if j>i:  ax[i,j].set_axis_off()\nax[3,0].set_xlabel('log10(ms)'); \nax[3,1].set_xlabel('log10(mh)'); \nax[3,2].set_xlabel('log10(rd)'); \nax[3,3].set_xlabel('log10(cc)'); \n\n\n\n\n\n\n\nTo finish, let’s have a look at how we can use the autoencoder to generate new fake data that resembles our original dataset. We do so by random sampling from the distribution of coded values that we obtained during training. In this way we generate a new plausible code which we then decode to construct a new rotation curve.\nWe start by generating new code from the distribution obtained during training - to do this we use numpy.random.choice\n\nsize=500\n\nnew_pp_ae = []\nfor i in range(4): new_pp_ae.append(tensor(np.random.choice(pp_ae[:,i].numpy(), size)))\nnew_code = torch.stack(new_pp_ae).T\n\nLet’s plot the original and new distributions of coded parameters:\n\nfig,ax = plt.subplots(figsize=(12,3), ncols=4)\n\nbins=[np.linspace(-10,10,50), np.linspace(-1,20,50), np.linspace(-2.5,2.5,50), np.linspace(-5,5,50)]\nfor i in range(4):\n    ax[i].hist(pp_ae[:,i].numpy(), bins=bins[i], density=True, label='data');\n    ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype='step', lw=2, density=True, label='new code');\n    if i==1: ax[i].legend(loc='upper right', frameon=False)\n\n\n\n\nSince they look very much alike we can now decode the new code that we just generated and we are ready to plot the new rotation curves\n\nfig,ax = plt.subplots(figsize=(6,4))\nfor v in datascale(ae.decoder(new_code),xmean,xstd): ax.plot(cm.rad, v.detach().numpy())\nax.set_xlabel('radius')\nax.set_ylabel('velocity');\n\n\n\n\nWith this method we have effectively generated new rotation curves that are realistic and are not part of the training dataset. This illustrates the power of autoencoders, however to do this even better we can adapt our autoencoder to learn the underlying distribution in the code space - this is what Variatioal Autoencoders (VAEs) are for!"
  },
  {
    "objectID": "posts/2022-03-18-neurons-non-linearities.html",
    "href": "posts/2022-03-18-neurons-non-linearities.html",
    "title": "Understanding how basic linear NNs handle non-linearities",
    "section": "",
    "text": "While studying deep learning, coming from a mathematical physics background, I struggled a lot in understanding how a neural network (NN) can fit non-linear functions. Most of the visual explanations or tutorials I could find on NNs did not give me a satisfactory explanation as to how a composition of linear functions - i.e. an NN - can handle non-linear behaviour. So I looked for more formal derivations, as I knew that the mathematical foundations of deep learning had to be sound.\nEventually I came across one of the most important papers in this field: Cybenko (1989, doi:10.1007/BF02551274). Reading this paper opened my eyes and gave me a completely different perspective on the problem. I realized that the key to the universal approximation theorem is that the composition of a linear function and a sigmoidal (so-called activation) function yields a series of functions which is dense in the space of continuous functions.\nIn other words, any continuous function can be written as a finite sum of terms given by the composition of a linear and a sigmoidal function, i.e. \\[\n\\sum_{i=0}^N \\alpha_i\\,\\sigma({\\bf w}_i\\cdot{\\bf x} + b_i),\n\\] with \\(\\sigma: \\mathrm{R}\\to\\mathrm{R}\\) being a sigmoidal activation function, \\({\\bf x}\\in\\mathrm{R}^n\\) and \\({\\bf w}_i\\in\\mathrm{R}^n\\), \\(\\alpha_i,\\,b_i\\in\\mathrm{R}\\) \\(\\forall i\\). Cybenko (1989) showed that the set of functions above spans the whole space of continuous functions in \\(\\mathrm{R}^n\\), effectively making this set kind of a basis for \\(\\mathrm{R}^n\\), except that the functions are not linearly independent.\nElements of this set of function as in the equation above are usually called units or neurons.\n\n\n\nSince a neuron is a composition of a linear function with an activation function, the key to approximate non-linear functions is in the sigmoidal activation function. Formally a sigmoidal is a function \\(\\sigma: \\mathrm{R}\\to\\mathrm{R}\\) such that \\(\\lim_{x\\to+\\infty} \\sigma(x)=1\\) and \\(\\lim_{x\\to-\\infty} \\sigma(x)=0\\).  The Heaviside function is an example of one of the simplest sigmoidal functions; however that is not continuous near \\(x=0\\), thus in practice smooth approximations of it are often used. A popular one is: \\[\n\\sigma(x)=\\frac{1}{1+e^{-x}}\n\\]\nBut how can a linear combination of lines composed with a step function approximate any non-linear behaviour? I knew from Cybenko’s results that this had to be the case, so I set out and tried to understand and see this better.\n\n\n\nI asked myself how many neurons (i.e. elements of Cybenko’s quasi-basis) would I need to approximate a simple second-order non-linearity, i.e. the function \\(x\\mapsto x^2\\).\n\n\nCode\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(device)\n\n%matplotlib inline\n%config Completer.use_jedi = False\n\n\nLet’s generate the data of a nice parabolic curve with some small random noise and let’s plot them\n\n# Training data\nsize = 500\nx = torch.linspace(-5, 5, size)\ny = x.pow(2) + 0.5 * torch.rand_like(x)\n\nplt.plot(x,y,'k.')\n\n\n\n\nIn order to build some intuition of how we may approximate this data with a linear combination of neurons let’s experiment a bit and overplot a few simple combinations of neurons by hand.\nFirst, we need to define our sigmoidal activation function:\n\nsig = lambda x: 1.0/(1.0+np.exp(-x))\n\n\nfig,ax = plt.subplots(figsize=(15,4), ncols=3)\n\ndef commons(ax):\n    ax.plot(x,y,'k.', zorder=0)\n    ax.legend(loc='upper center')\n    ax.set_ylim((None, 30))\n\n\nax[0].plot(x,20*sig(x), lw=3, label=r\"$\\rm 20\\sigma(x)$\")\nax[0].plot(x,20*sig(-x), lw=3, label=r\"$\\rm 20\\sigma(-x)$\")\nax[0].plot(x,20*(sig(x)+sig(-x)), ls='--', c='tab:green', label=r\"$\\rm 20[\\sigma(x)+\\sigma(-x)]$\")\ncommons(ax[0])\n\nax[1].plot(x,20*sig(x-3), lw=3, label=r\"$\\rm 20\\sigma(x-3)$\")\nax[1].plot(x,20*sig(-x-3), lw=3, label=r\"$\\rm 20\\sigma(x-3)$\")\nax[1].plot(x,20*(sig(x-3)+sig(-x-3)), ls='--', c='tab:green', label=r\"$\\rm 20[\\sigma(x-3)+\\sigma(-x-3)]$\")\ncommons(ax[1])\n\nax[2].plot(x,25*(sig(1.2*x-4)), lw=3, label=r\"$\\rm 25\\sigma(1.2x-4)$\")\nax[2].plot(x,25*(sig(-1.2*x-4)), lw=3, label=r\"$\\rm 25\\sigma(-1.2x-4)$\")\nax[2].plot(x,25*(sig(1.2*x-4)+sig(-1.2*x-4)), ls='--', c='tab:green',\n           label=r\"$\\rm 25[\\sigma(1.2x-4)+\\sigma(-1.2x-4)]$\")\ncommons(ax[2])\n\n\n\n\nThese examples help in building up some intuition on how we can use these sigmoidal building blocks to approximate relatively simple non linear behaviours. In this specific case of a convex second-order non-linearity, the sum of two scaled, shifted and mirrored step functions seems to yield a decent representation of the data close to the origin.\n\n\nNaturally, this is strongly dependent on the particular shape of the sigmoidal function that we chose, i.e. \\(\\sigma: x\\mapsto (1+e^{-x})^{-1}\\). If we had chosen a Heaviside function instead, the sum of the two neurons above would not have yielded a similarly good approximation of the data.\n\n\nCode\nfig,ax=plt.subplots()\nax.plot(x,25*(np.heaviside(1.2*x-4,0.5)), lw=3, label=r\"$\\rm 25\\sigma(1.2x-4)$\")\nax.plot(x,25*(np.heaviside(-1.2*x-4,0.5)), lw=3, label=r\"$\\rm 25\\sigma(-1.2x-4)$\")\nax.plot(x,25*(np.heaviside(1.2*x-4,0.5)+np.heaviside(-1.2*x-4,0.5)), ls='--', c='tab:green',\n        label=r\"$\\rm 25[\\sigma(1.2x-4)+\\sigma(-1.2x-4)]$\")\ncommons(ax)\n\n\n\n\n\n\n\n\nNow that we have some intuition on the linear combinations of neurons let’s try to answer the question posed at the beginning of Part 1, that is how many neurons are needed to approximate \\(x^2\\).\nTo answer this we will build a series of simple torch models made up of just one (hidden) layer of neurons, i.e.\nnn.Sequential(nn.Linear(1, N_units), nn.Sigmoid(), nn.Linear(N_units, 1))\nWe start by defining a learning rate and a number of epochs; then we loop through the 5 numbers of neurons explored, [1,2,4,8,16], we set up the nn.Sequential model and we start the full training loop on the data. We put all of this into a convenient class with a run method and a plots method, which we use to visualize the output.\n\n\nCode\nclass ModelsTestingActivations():\n    def __init__(self, activation_fn=nn.Sigmoid(), loss_fn=nn.MSELoss(reduction='sum'), \n                 units=[1,2,4,8,16], learning_rate=3e-2, num_epochs=1000):\n        self.activ_fn, self.loss_fn = activation_fn, loss_fn\n        self.units, self.lr, self.num_epochs = units, learning_rate, num_epochs\n        # outputs\n        self.models, self.preds = [], []\n        \n    def make_model(self, u):\n        return nn.Sequential(nn.Linear(in_features=1, out_features=u, bias=True), \n                             self.activ_fn,\n                             nn.Linear(in_features=u, out_features=1)\n                            )\n    \n    def plots(self, residuals=True, xextrap=10):\n        if not hasattr(self, 'x'): \n            print ('Have you run the model yet?')\n            return\n       \n        fig,ax = plt.subplots(figsize=(18,3.2), ncols=len(self.units))\n        for i in range(len(self.units)):\n            ax[i].set_xlabel(r'$x$')\n            if i==0: ax[i].set_ylabel(r'$y$')\n            ax[i].plot(self.x,self.y,'k.')\n            ax[i].plot(self.x,self.preds[i],'r.')\n            ax[i].plot(np.linspace(-xextrap,xextrap), \n                       self.models[i](torch.linspace(-xextrap,xextrap,50).unsqueeze(1)).detach(), 'b--')\n            ax[i].text(0.05,0.05,r\"N=%d\" % self.units[i], transform=ax[i].transAxes, fontsize=14)\n\n        # residuals\n        if not residuals: return\n        fig,ax = plt.subplots(figsize=(18,1.6), ncols=len(self.units))\n        for i in range(len(self.units)):\n            ax[i].set_xlabel(r'$x$')\n            if i==0: ax[i].set_ylabel(r'$\\Delta y$')\n            ax[i].plot(self.x,(self.y-self.preds[i]).abs(), 'k-', lw=0.5)\n            ax[i].text(0.5,0.85, r\"$\\rm \\langle\\Delta y\\rangle=%1.2f$\" % (self.y-self.preds[i]).abs().mean(), \n                       transform=ax[i].transAxes, fontsize=12, ha='center', va='center')\n    \n    def run(self, x, y):\n        self.x, self.y = x, y \n        for i,u in enumerate(self.units):\n            # define model\n            model = self.make_model(u)\n            self.models.append(model)\n\n            # define optimizer\n            optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n\n            # fitting loop\n            for epoch in range(self.num_epochs):\n                # reinitialize gradient of the model weights\n                optimizer.zero_grad()\n\n                # prediction & loss\n                y_pred = model(self.x.unsqueeze(-1))\n                loss = self.loss_fn(y_pred, self.y.unsqueeze(-1))\n\n                # backpropagation\n                loss.backward()\n\n                # weight update\n                optimizer.step()\n\n            self.preds.append(y_pred.squeeze().detach())\n\n\nIt is interesting to see how the results of this series of models change depending on which activation function is used, thus we can make our custom class to have as input the shape of the activation function.\nWe start with the classic sigmoidal\n\nmods_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=5e-2,\n                                        num_epochs=2000)\nmods_sigmoid.run(x, y)\n\n\nmods_sigmoid.plots()\n\n\n\n\n\n\n\nThese plots show how the NN model (red) compares to the actual data used for training (black) as a function of the number (N) of neurons used in the linear layer. Moving from the panels left to right the number of neurons increases from \\(N=1\\) to \\(N=6\\). The dashed blue line is the prediction of the NN model, which we also extrapolated in the range \\(x\\in[-10,10]\\) outside of the domain of the data, that are confined within \\([-5,5]\\). The rows of panels below show the residuals \\(\\Delta y=|y-y_{\\rm NN}|\\), i.e. abs(black-red), while \\(\\langle \\Delta y \\rangle\\) is the mean of the residuals.\nThere are a few interesting things that we can notice: - when using only one neuron the NN is able only to capture the mean of the data \\(\\langle y \\rangle \\approx 8.613\\) - with N=2 neurons, the linear combinations of the two activations is already able to represent the convex 2nd-order non-linearity of the data, reducing the mean residual by an order of magnitude with respect to the model just predicting the mean (i.e. that with N=1). - obviously, increasing N results in a better approximation to the training data, for a fixed learning rate and number of training epochs, up to a residual 4x better with N=6 than with N=2. - the extrapolations of the NN models outside of the domain of the data do not follow the \\(x^2\\) curve at all, showing that the NN models have successfully learned to reproduce the data, but have not learned completely the behaviour of the underlying curve. This exemplifies that NN models are often poor predictors outside of the domain of the training data\nWe can have a closer look at the parameters obtained by the NN model with 2 neurons and see how do they compare to the simple fit by eye that we did above. To do this we can grab the named_parameters of the model with \\(N=2\\) and print them out\n\nfor name, param in mods_sigmoid.models[1].named_parameters():\n    if param.requires_grad:\n        print (name, param.data)\n\n0.weight tensor([[-1.4160],\n        [ 1.4624]])\n0.bias tensor([-4.9017, -4.9822])\n2.weight tensor([[23.6837, 22.9339]])\n2.bias tensor([0.9818])\n\n\n\n\n\n\nCode\ndef string_func_mod(x):\n    return (\"%1.0f sig(%1.1f*x %1.1f) + %1.0f sig(%1.1f*x %1.1f) + %1.1f\" %\n           (x[2], x[0], x[1], x[5], x[3], x[4], x[6]))\n\nprint(string_func_mod(mod2_sigmoid))\n\n\n24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0\n\n\nReally similar to the parameters we obained by eye! (results may vary a bit due to randomness)\n\n\n\nWe can also repeat this exercise with different activation functions, e.g. with a softsign\n\nmods_softsign = ModelsTestingActivations(activation_fn=nn.Softsign(), units=[1,2,3,4,6], learning_rate=9e-2,\n                                        num_epochs=1000)\nmods_softsign.run(x, y)\n\n\nmods_softsign.plots()\n\n\n\n\n\n\n\nor with a ReLU (however, ReLU is not sigmoidal in the sense of Cybenko, so strictly speaking their universal approximation theorem does not apply. It has been shown that the hypothesis on \\(\\sigma\\) can be relaxed to it being non-constant, bounded and piecewise continuous, see e.g. Hornik 1991, Leshno et al. 1993)\n\nmods_relu = ModelsTestingActivations(activation_fn=nn.ReLU(), units=[1,2,3,4,6], learning_rate=5e-2,\n                                        num_epochs=2000)\nmods_relu.run(x, y)\n\n\nmods_relu.plots()\n\n\n\n\n\n\n\nWhile the accuracy of the NN with ReLU, as measured by \\(\\langle\\Delta y\\rangle\\), is not significantly better than with Sigmoid for the sake of this experiment, the extrapolation out-of-domain is much better. This is because the NN model with ReLU tends to linear outside of the data domain, while the NN model with Sigmoid is approximately constant outside of the range of the training data.\n\n\n\n\n\nIt would be interesting to fully reconstruct the likelihood distribution of the parameters of the NN model. If the number of parameters is not huge - that is if we are working with a limited number of neurons in a single layer - then an exploration of the multi-dimensional likelihood distribution is still feasible. Moreover, if we are able to map the full likelihood of the model parameters we can also see where the best model found by the NN sits in the space of the likelihood.\nTo do this we can use a Monte Carlo Markov Chain (MCMC) analysis. This is actually what I would normally do when facing an optimization problem in physics, as often one can have a pretty good guess on a suitable functional form to use for the fitting function and, more importantly, this method naturally allows to study the uncertainties on the model found.\nWe’re using the library emcee (paper) to run the MCMC analysis and corner (paper) to plot the posterior distribution.\n\n\nCode\nimport emcee\nimport corner\n\n# the functional definition of the NN model\ndef lmod(x, pars):\n    \"\"\" A linear combination of nu sigmoidals composed with linear functions \"\"\"\n    nu = int((len(pars)-1)/3) # number of neurons, with 2 biases\n    res = pars[-1]\n    for i in range(nu): res += sig(pars[0+i*3]*x+pars[1+i*3])*pars[2+i*3]\n    return res \n\n# log-likelihood\ndef lnlike(pars, x, y):\n    \"\"\" This is equivalent to MSELoss(reduction='sum') \"\"\"\n    y_lmod = lmod(x,pars)\n    return -((y-y_lmod).pow(2).sum() / len(y)).item()\n\n# log-prior on the parameters\ndef lnprior(pars):\n    \"\"\" A multi-dimensional Gaussian prior with null mean, fixed std and null correlation \"\"\"\n    std = 30\n    lp = 0.\n    for p in pars: lp += -0.5*(p)**2/std**2-np.log(np.sqrt(2*np.pi)*std)\n    return lp\n\n# log-probability = log-likelihood + log-prior\ndef lnprob(pars, x, y):\n    lk, lp = lnlike(pars, x, y), lnprior(pars)\n    if not np.isfinite(lp) or not np.isfinite(lk):\n        return -np.inf\n    return lp + lk\n\n\nLet’s run the MCMC analysis for a \\(N=2\\) NN (see here for a bit more context on MCMCs with emcee)\n\nnunits = 2\ndim = 3*nunits+1\n\n\n%%time\nnu, nw, nstep = 2, 4*dim, 10000\n\n# initial conditions of each chain\npos = [[0]*dim + 1e-4*np.random.randn(dim) for j in range(nw)]\n\n# launch the MCMC\nsampler = emcee.EnsembleSampler(nw, dim, lnprob, args=(x.squeeze(), y.squeeze()))\nsampler.run_mcmc(pos, nstep);\n\n# collate the chains of each walker and remove the first 500 steps - the burn-in phase\nsamples = sampler.chain[:,500:,:].reshape((-1, dim))\n\n/Users/lposti/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n  \"\"\"Entry point for launching an IPython kernel.\n\n\nCPU times: user 1min 23s, sys: 988 ms, total: 1min 24s\nWall time: 1min 38s\n\n\nFinally, let’s plot the posterior probability distribution of the 7 parameters of a NN model with \\(N=2\\) neurons and let’s also mark the location of the model we obtained above with nn.Sequential\n\ncorner.corner(samples, bins=30, smooth=1.5, smooth1d=1.5, truths=mod2_sigmoid);\n\n\n\n\nFind the maximum probability model, i.e. the model with highest log-prob\n\nidmax = np.unravel_index(sampler.lnprobability.argmax(), sampler.lnprobability.shape) # maximum probability\nmax_prob = sampler.chain[idmax[0],idmax[1],:]\n\n\nprint ('NN model:')\nprint (string_func_mod(mod2_sigmoid), '    LOSS:%1.2f' % lnlike(mod2_sigmoid, x, y))\n\nprint ()\nprint ('MCMC model')\nprint (string_func_mod(max_prob), '    LOSS:%1.2f' % lnlike(max_prob, x, y))\n\nNN model:\n24 sig(-1.4*x -4.9) + 23 sig(1.5*x -5.0) + 1.0     LOSS:-0.73\n\nMCMC model\n28 sig(-1.2*x -4.3) + 31 sig(1.0*x -3.9) + -0.2     LOSS:-0.31\n\n\nand we can also plot them side by side in comparison to the data\n\nfig,ax = plt.subplots(figsize=(9,4.), ncols=2)\n\ndef commons(ax):\n    ax.set_xlabel(r'$x$')\n    ax.set_ylabel(r'$y$')\n    ax.plot(x,y,'ko')\n    ax.set_ylim((-1,32))\n    \ncommons(ax[0])\nax[0].set_title(\"NN best model\", fontsize=16)\nax[0].plot(x, lmod(x, mod2_sigmoid), 'r.')\nax[0].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), mod2_sigmoid), 'b--')\n\ncommons(ax[1])\nax[1].set_title(\"MCMC max prob\", fontsize=16)\nax[1].plot(x, lmod(x, max_prob), 'r.')\nax[1].plot(np.linspace(-10,10), lmod(np.linspace(-10,10), max_prob), 'b--')\n\n\nfig,ax = plt.subplots(figsize=(9,2.), ncols=2)\n\ndef commons(ax):\n    ax.set_xlabel(r'$x$')\n    ax.set_ylabel(r'$\\Delta y$')\n    ax.set_ylim((-0.1,3))\n    \ncommons(ax[0])\nax[0].plot(x, (y-lmod(x, mod2_sigmoid)).abs(), 'k-', lw=0.5)\nax[0].text(0.5,0.85, r\"$\\rm \\langle\\Delta y\\rangle=%1.2f$\" % (y-lmod(x, mod2_sigmoid)).abs().mean(), \n               transform=ax[0].transAxes, fontsize=12, ha='center', va='center');\ncommons(ax[1])\nax[1].plot(x, (y-lmod(x, max_prob)).abs(), 'k-', lw=0.5)\nax[1].text(0.5,0.85, r\"$\\rm \\langle\\Delta y\\rangle=%1.2f$\" % (y-lmod(x, max_prob)).abs().mean(), \n               transform=ax[1].transAxes, fontsize=12, ha='center', va='center');\n\n\n\n\n\n\n\n\n\n\nLet’s repeat the analysis of Part 1 but for a more complex non-linear function, for instance a sin function with an oscillating non-linear behaviour. Let’s now ask ourselves how many neurons would you need to fit this function.\nAs before, let’s start by generating some data\n\n# Training data\nsize = 1000\nx = torch.linspace(-10, 10, size)\ny = torch.sin(x) + 0.2 * torch.rand_like(x)\n\nplt.plot(x,y,'k.')\n\n\n\n\nNow we can use the same ModelsTestingActivations class that we wrote above, just passing the new xs and ys to the run method. Let’s use a Sigmoid activation function and let’s have a look at the performance of the NN models for increasing number of neurons \\(N\\)\n\nmods_sin_sigmoid = ModelsTestingActivations(activation_fn=nn.Sigmoid(), units=[1,2,3,4,6], learning_rate=4e-2,\n                                            num_epochs=2000)\nmods_sin_sigmoid.run(x, y)\n\n\nmods_sin_sigmoid.plots(xextrap=20)\n\n\n\n\n\n\n\nFrom these plots we can clearly notice a few important things: - the number of neurons limits the number turnovers (i.e. changes of sign of the derivative) of the function that can be fitted - an NN model with \\(N\\) neurons is generally limited to approximate decently only function that change their increasing/decreasing tendency \\(N\\) times - in this particular example, the sin function turnsover 6 times in the interval \\([-10,10]\\), thus an NN with at least \\(N=6\\) neurons is needed to capture all the times the data turnover - also in this case, the extrapolation of the NN models outside of the domain of the data yield poor predictions. This means that the NN has learned to reproduce the data, but has not learned the underlying functional behaivour"
  },
  {
    "objectID": "posts/2022-10-17-gaussian-processes.html",
    "href": "posts/2022-10-17-gaussian-processes.html",
    "title": "Gaussian Processes: modelling correlated noise in a dataset",
    "section": "",
    "text": "A few weeks ago I saw a paper passing on the arXiv by Aigrain & Foreman-Mackey which is a review on Gaussian Processes (GPs) from an astrophysical perspective - it is actually targeted for astronomical time-series. I had heard a lot of things about GPs before, but I never really had the time nor will to sit down an actually understand what they are about. That changed when I glanced over the review by Aigrain & Foreman-Mackey, as I quickly realised a couple of things that caught my interest.\nIn particular, the thing that struck me the most is their Figure 3. This appears at the end of the first Section of the review, which is mostly dedicated to two motivating examples from astronomical time-series analysis (a very well-crafted first Section I must say!). This shows the fit of a mock exoplanet transit lightcurve by two models: the both share the same physics, which is also used to generate the mock dataset, but one takes into account the correlated noise in the data, while the other doesn’t and therefore works under the assumption that the datapoints are all uncorrelated and independent.\nGPs are used to realistically represent the correlation in the time-series data typically observed for exoplanet transits and are an ingredient used to generate the mock dataset. So the final outcome of this controlled experiment is that only the model that accounts for correlated data, modelled with a GP, is able to recover reasonably well the true physical parameters, while the other model without a GP infers a severely biased result.\nIt is interesting to notice that in this example the GP introduces another layer of uncertainty in the model (i.e. how are the data correlated) over which we have to marginalise in order to arrive at the final prediction for the physical parameters. This means that the simple model that treats the data as independent is very confidently inferring a biased result (with relatively high accuracy), as opposed to the model with a GP which is instead less accurate, but unbiased."
  },
  {
    "objectID": "posts/2022-10-17-gaussian-processes.html#gaussian-processes-gps-in-rotation-curve-modelling",
    "href": "posts/2022-10-17-gaussian-processes.html#gaussian-processes-gps-in-rotation-curve-modelling",
    "title": "Gaussian Processes: modelling correlated noise in a dataset",
    "section": "Gaussian Processes (GPs) in rotation curve modelling",
    "text": "Gaussian Processes (GPs) in rotation curve modelling\nIn this notebook I’m going to explore how we can use GPs in modelling rotation curves of galaxies. Often - pretty much always - circular velocity measurements in galaxies are treated as independent and parametrized rotation curve models are fitted to such datasets without worrying too much if that is a reasonable assumption. Given that most HI rotation curves are derived with a tilted-ring model, I am unconfortable with assuming that each datapoint in a rotation curve is actually independent from all the others, since adjacent rings can easily enter the line-of-sight of a given radius.\nFor this reason I am going to generate a mock rotation curve dataset where the points are actually correlated. This will be done using a GP. Then I will be fitting this dataset with two models: one assuming that the data are independent, and another taking into account the correlated noise.\n\n\nCode\nimport numpy as np\nimport matplotlib\nimport matplotlib.pylab as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport random\nimport jax\nimport jax.numpy as jnp\nimport jaxopt\nfrom functools import partial\nfrom tinygp import GaussianProcess, kernels\nimport numpyro\nimport arviz\nimport corner\n\njax.config.update(\"jax_enable_x64\", True)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%config Completer.use_jedi = False\n%matplotlib inline\n\nrng = np.random.default_rng()\n\n\n\nDefinitions and rotation curve sampling\nDefine the mathematical functions for the galaxy rotation curves\n\nG, H, Dc = 4.301e-9, 70, 200.\n\n# accessory functions\ndef jax_fc(x): return jnp.log(1+x)-x/(1+x)\ndef jax_Vvir(Mh): return jnp.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.))\ndef jax_Rvir(Mh):\n    rho_hat = 4. / 3. * np.pi * Dc * (3. * (H)**2 / (8. * np.pi * G))\n    return 1e3 * ((Mh / rho_hat)**(1./3.))\n\n# actual rotation curve\ndef jax_vhalo(params, R):\n    Mh, cc = 10**params['log_mh'], 10**params['log_c']    \n    rv = jax_Rvir(Mh)\n    return jnp.sqrt(jax_Vvir(Mh)**2*rv/R*jax_fc(cc*R/rv)/jax_fc(cc)) \n\nLat’s now plot a randomly sampled rotation curve with a typical error on each datapoint of 8 km/s. As a first step, we will assume that each datapoint is independent and thus we will sample from a Gaussian noise distribution for each measurement.\n\nsize = 40\nlmh, lc, verr = 12, 0.95, 10.0\n\n# generating independent datapoints with Gaussian errors\nx = np.linspace(0.1, 50, size)\ny = jax_vhalo({'log_mh':lmh, 'log_c':lc}, x) + rng.normal(loc=0, scale=verr, size=size)\n\nfig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={'height_ratios':(0.65,0.35)})\n# rotation curve\nax[0].plot(x, jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), '--', c='tab:pink', label='true mean')\nax[0].errorbar(x, y, yerr=verr, fmt='.', c='k', lw=0.5, label='data')\nax[0].legend(loc='lower right')\nax[0].set_ylabel('velocity');\n# residuals\nax[1].axhline(y=0, ls='--', c='tab:pink')\nax[1].errorbar(x, y-jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), yerr=verr, fmt='.', c='k', lw=0.5)\nax[1].set_xlabel('radius')\nax[1].set_ylabel('residuals')\nax[1].set_ylim(-60,60);\n\n\n\n\nIn the simple case plotted above we have generated the points with the implicit assumption that each datapoint was independent from all the others. This is why mathematically we simply added a Gaussian noise term to the median curve when defining y. In the residuals plot, the fact that each datapoint is independent becomes apparent since there is no clear trend in the residuals as a function of radius.\nHowever, in practice this is rarely the case with astronomical observations, since typically instrumental characteristics of the telescope and physical processes make the measurement of a single datapoint to have a non-negligible dependence on some other datapoints. Most of the times when modelling astrophysical data we do not know precisely if and which measurements are correlated with which others, so it is in our best interest to employ a modelling technique that allows for correlated datapoints, instead of assuming they are independent. This is where GPs come into play.\n\n\nGenerating rotation curve data with correlated noise using GPs\nLet’s now use GPs to generate a new set of datapoints, but this time they will be correlated to one another. To specify this correlation we need to define a kernel or a covariance function which, in the simplest case that we are using here, is a function only of the physical distance of each point (absolute or L1 distance).\nKernels that depend only of the distance of points are called stationary. A very common kernel function used in GPs is the so-called radial basis function (RBF) or exponential-squared, since \\(k(x_i, x_j) \\propto \\exp\\left[-\\frac{1}{2}(d_{ij}/s)^2\\right]\\), where \\(d_{ij}=|x_i-x_j|\\) is the distance of the two datapoints, while \\(s\\in\\mathbb{R}\\) is a scale parameter.\n\nBuild the GP with tinygp\nWe define the GP as follows using the library tinygp.\n\ndef build_gp(params, x, yerr):\n    kernel = 10**params['log_amp']*kernels.ExpSquared(10**params['log_scl'], distance=kernels.distance.L1Distance())\n    return GaussianProcess(kernel, \n                           x, \n                           diag=yerr**2, \n                           mean=partial(jax_vhalo, params)\n                          )\n\nThis GP has 2 adjustable parameters: an amplitude log_amp and a scale log_scl (both defined in log). We build the GP by passing it the kernel function, the set of datapoints (just x, not the velocity measurements), the measured uncertainty of the measurements, and the mean function that needs to be added to the noise generated by the GP.\nWhat the library is doing is just building a full covariance matrix on the dataset x using the kernel function provided. The value that we pass on the diag argument will be considered as an additional variance to be added to the covariance matrix.\nLet’s now initialize a GP with some amplitude and scale parameters and let’s sample random datapoints from its covariance matrix.\n\nparams = {'log_mh':lmh, 'log_c':lc,\n          'log_amp':jnp.log10(300.0), 'log_scl':jnp.log10(5.0)}\n\n# initialize the GP and sample from it\ngp   = build_gp(params, x, verr)\n# vm   = gp.sample(jax.random.PRNGKey(11))\nvm   = gp.sample(jax.random.PRNGKey(33))\ne_vm = np.sqrt(gp.variance)\n\nHere gp.sample gets a random realization of y-measurements on the x-array. We can define their standard errorbars by just taking the variances (i.e. diagonal of the covariance matrix).\n\n\nThe covariance matrix\nJust to help out with visualising the GP, let’s plot the covariance matrix for this problem.\n\ndef plt_mat(ax, params, x, yerr):\n\n    gp = build_gp(params, x, yerr)\n    \n    xm1, xm2 = jnp.meshgrid(x,x)\n    zm = np.zeros_like(xm1.flatten())\n    for i in range(len(xm1.flatten())):\n        zm[i]=(gp.kernel.evaluate(xm1.flatten()[i], xm2.flatten()[i]))\n        \n    im=ax.matshow(zm.reshape((len(x), len(x))), extent=(x.min(), x.max(), x.max(), x.min()))\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n\n    plt.colorbar(im, cax=cax)\n\n\nfig,ax = plt.subplots()\nplt_mat(ax, params, x, verr)\n\n\n\n\nHere each point is coloured according to the covariance \\(k(x_i,x_j)\\). It is highest along the diagonal, where \\(k(x_i,x_i)=300\\) km\\(^2\\)/s\\(^2\\), implying a standard uncertainty on each data point of \\(\\sigma=\\sqrt{300+10^2}=20\\) km/s, where the term \\(10^2\\) comes from adding the measured uncertainties to the covariance matrix (the diag argument in GaussianProcess). Given the kernel function and the scale of \\(5\\) that we used in this example, we can see that each datapoint has a significant correlation with all the points closer than ~10.\n\n\nSampling the rotation curve with correlated noise\nFinally, let’s plot the sampled rotation curve and its residuals and let’s compare them with the uncorrelated case above.\n\nfig,ax = plt.subplots(figsize=(5,5), nrows=2, gridspec_kw={'height_ratios':(0.65,0.35)})\n# rotation curve\nax[0].plot(x, jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), '--', c='tab:pink', label='true mean')\nax[0].errorbar(x, vm, yerr=e_vm, fmt='.', c='C0', lw=0.5, label='data')\nax[0].legend(loc='lower right')\nax[0].set_ylabel('velocity');\n# residuals\nax[1].axhline(y=0, ls='--', c='tab:pink')\nax[1].errorbar(x, vm-jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), yerr=e_vm, fmt='.', c='C0', lw=0.5)\nax[1].set_xlabel('radius')\nax[1].set_ylabel('residuals')\nax[1].set_ylim(-60,60);\n\n\n\n\nWe do see quite a lot of structure in the residuals plot this time! This is in stark contrast to the picture we had when generating datapoints independently. This time each measurements feels the influence of the other measurements closer than ~10 in radius, thus the rotation curve starts having significant trends above and below the mean.\nWhen fitting the rotation curve these trend can be misinterpreted as signal, instead of just correlated noise, and this can potentially bias our inference on the curve parameters quite significantly. We see below an example of this.\n\n\n\nFitting the rotation curve with or without Gaussian Processes\nLet’s now consider the rotation curve generated with the GP above, i.e. the blue set of points, and let’s build a model to fit it. The model is the same jax_vhalo function that we used to generate the data, which has 2 free parameters: a mass log_mh and a concentration log_c.\nWe run the fit in a Bayesian framework and in particular with an MCMC sampler using a standard \\(\\chi^2\\) log-likelihood on the observed datapoints. We impose a uniform prior on log_mh and normal prior on log_c, whose mean follows the well-known mass-concentration relation of dark matter halos in \\(\\Lambda\\)CDM (Dutton & Maccio’ 2014).\nWe use the library numpyro to define the model and to run the MCMC sampling. In particular, numpyro uses a state-of-the-art Hamiltonian No U-Turn Sampler (NUTS) to derive the posterior of the parameters.\nWe run the fit two times: the first time, we treat the datapoints as independent and we have a “standard” Bayesian inference on the parameters; the second time, we allow the data to be correlated and we model their correlation with a GP with an exp-squared kernel that has two additional free parameters, an amplitude log_amp and a scale log_scl. We impose an uniformative uniform prior on the two parameters of the kernel.\n\nModel without GP\n\n# radial grid to define the output of the GP\nr_grid = jnp.linspace(0.1, 50.0, 1000)\n\ndef model(t, y_err, y, use_gp=False):\n    \n    # priors\n    log_mh = numpyro.sample('log_mh', numpyro.distributions.Uniform(8.0,  14.0)) \n    log_c  = numpyro.sample('log_c',  numpyro.distributions.Normal(0.905-0.101*(log_mh-12.0), 0.15)) \n    \n    # parameters of the underlying physical model\n    params = {\"log_mh\": log_mh, \n              \"log_c\" : log_c}\n    \n    if use_gp:\n        # branch WITH GPs\n        # \n        # define kernel parameters\n        params[\"log_amp\"] = numpyro.sample(\"log_amp\", numpyro.distributions.Uniform(-4.0, 5.0))\n        params[\"log_scl\"] = numpyro.sample(\"log_scl\", numpyro.distributions.Uniform(-2.0, 3.0))\n        \n        # generate the GP\n        gp = build_gp(params, t, y_err)\n        \n        # sample the posterior\n        numpyro.sample(\"y\", gp.numpyro_dist(), obs=y)\n        \n        # calculate properties of the model\n        mu = gp.mean_function(r_grid)\n        numpyro.deterministic(\"mu\", mu)\n        numpyro.deterministic(\"gp\", gp.condition(y, r_grid, include_mean=False).gp.loc)\n\n    else:\n        # branch WITHOUT GPs\n        #\n        # sample the posterior\n        numpyro.sample(\"y\", numpyro.distributions.Normal(jax_vhalo(params, t), y_err), obs=y)\n        \n        # calculate properties of the model\n        numpyro.deterministic(\"mu\", jax_vhalo(params, r_grid))\n\n\n\nSampling the posterior with numpyro\nThe model function above has the numpyro primitives like numpyro.sample that are used by the NUTS MCMC sampler to construct the posterior. Below we run the model the first time selecting the use_gp=False branch, i.e. assuming that the data are independent.\nWe launch 2 chains for 3000 steps (of which 1/3 of warmup) of the NUTS sampler, starting from a specific value of the parameters that is not too far from the truth (to convieniently speed up convergence). We then use the arviz package to evaluate some statistics on the posterior samples.\n\nsampler_wn = numpyro.infer.MCMC(\n    numpyro.infer.NUTS(\n        model,\n        dense_mass=True,\n        target_accept_prob=0.9,\n        init_strategy=numpyro.infer.init_to_value(values={'log_mh':11.0, 'log_c':1.0}),\n    ),\n    num_warmup=1000,\n    num_samples=3000,\n    num_chains=2,\n    progress_bar=True,\n)\n%time sampler_wn.run(jax.random.PRNGKey(11), x, e_vm, vm)\n\ninf_data_wn = arviz.from_numpyro(sampler_wn)\narviz.summary(inf_data_wn, var_names=[\"log_mh\", \"log_c\"])\n\nsample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11<00:00, 357.51it/s, 3 steps of size 8.43e-01. acc. prob=0.91]\nsample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:11<00:00, 357.45it/s, 3 steps of size 7.96e-01. acc. prob=0.93]\n\n\nCPU times: user 18.3 s, sys: 1.07 s, total: 19.4 s\nWall time: 23.4 s\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      log_mh\n      11.897\n      0.060\n      11.786\n      12.009\n      0.001\n      0.001\n      4581.0\n      3535.0\n      1.0\n    \n    \n      log_c\n      1.087\n      0.058\n      0.983\n      1.200\n      0.001\n      0.001\n      4758.0\n      3911.0\n      1.0\n    \n  \n\n\n\n\n\n\nExplore the MCMC samples with arviz\nThe posterior has been successfully sampled and we can now have a look at the marginalized distributions of the two physical parameters, mass and concentration.\n\narviz.plot_density(inf_data_wn, var_names=[\"log_mh\", \"log_c\"], hdi_prob=0.99, colors='k', shade=0.1);\n\n\n\n\n\n\nModel with GP\nLet’s now repeat the fitting procedure, but this time for the use_gp=True branch of model, i.e. allowing for the data to be correlated.\n\nsampler = numpyro.infer.MCMC(\n    numpyro.infer.NUTS(\n        model,\n        dense_mass=True,\n        target_accept_prob=0.9,\n        init_strategy=numpyro.infer.init_to_value(values={'log_mh':11.0, 'log_c':1.0, \n                                                          'log_amp':1.0, 'log_scl':0.5\n                                                         }),\n    ),\n    num_warmup=1000,\n    num_samples=3000,\n    num_chains=2,\n    progress_bar=True,\n)\n%time sampler.run(jax.random.PRNGKey(11), x, e_vm, vm, use_gp=True)\n\ninf_data = arviz.from_numpyro(sampler)\narviz.summary(inf_data, var_names=[\"log_mh\", \"log_c\", \"log_amp\", \"log_scl\"])\n\nsample: 100%|████████████████████████████████████████████████████████████████████| 4000/4000 [00:21<00:00, 183.03it/s, 7 steps of size 2.41e-01. acc. prob=0.97]\nsample: 100%|███████████████████████████████████████████████████████████████████| 4000/4000 [00:14<00:00, 273.18it/s, 31 steps of size 1.56e-01. acc. prob=0.97]\n\n\nCPU times: user 1min, sys: 2.38 s, total: 1min 2s\nWall time: 44.9 s\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      log_mh\n      11.974\n      0.261\n      11.525\n      12.476\n      0.006\n      0.004\n      2440.0\n      1578.0\n      1.0\n    \n    \n      log_c\n      0.989\n      0.142\n      0.716\n      1.258\n      0.003\n      0.002\n      2553.0\n      2809.0\n      1.0\n    \n    \n      log_amp\n      2.723\n      0.647\n      1.929\n      3.712\n      0.039\n      0.027\n      942.0\n      580.0\n      1.0\n    \n    \n      log_scl\n      0.840\n      0.233\n      0.517\n      1.143\n      0.008\n      0.008\n      1543.0\n      1419.0\n      1.0\n    \n  \n\n\n\n\nIn this case, we have two additional free parameters that are the amplitude and scale of the GP kernel. These can be considered as nuisance parameters in the present case, since we are only interested in the distributions of the two physical parameters marginalized over everything else.\nThe chain statistics summarized above look great, but let’s inspect the plot of the autocorrelation time to be extra sure that all the chains are converged and well-behaved.\n\narviz.plot_autocorr(inf_data, var_names=[\"log_mh\", \"log_c\"], max_lag=200);\n\n\n\n\nFrom this plot we can see that the autocorrelation of both parameters for all chains tends to die out for sufficiently large lags. This confirms that the MCMC samples that we have derived are actually independent and can be reliably used to infer the posterior.\nLet’s now compare the marginalized distributions of log_mh and log_c between the two modeling runs.\n\narviz.plot_density([inf_data_wn, inf_data], var_names=[\"log_mh\", \"log_c\"],\n                   data_labels=[\"independent data\", \"correlated data\"],\n                   hdi_prob=0.99, colors=['k','C0'], shade=0.1);\n\n\n\n\nWe clearly see that if we working under the assumption of uncorrelated data the resulting posteriors are thinner and somewhat biased. On the other hand, by allowing the data to be correlated in the fit, as modelled by a GP, the resulting posterior are significantly wider and more uncertain, but are significantly less biased.\n\n\nComparing the 2 models with a corner plot\nLet’s have a look at the corner plot of the marginalised posterior distribution in the mass-concentration space.\n\nranges = [(11.4, 12.6), (0.5, 1.4)] # PRNG 33\nfig = corner.corner(inf_data_wn, bins=40, range=ranges, \n                    color=\"k\", var_names=[\"log_mh\", \"log_c\"], smooth=1.0, smooth1d=1.0\n                   )\nfig = corner.corner(inf_data, bins=40, range=ranges, \n                    color=\"C0\", var_names=[\"log_mh\", \"log_c\"], smooth=1.0, smooth1d=1.0,\n                    labels=[\"$\\log\\,M_h$\", \"$\\log\\,c$\"],\n                    truths=[params['log_mh'], params['log_c']], truth_color='tab:pink',\n                    fig=fig)\n\n# make legend\nax = fig.axes[1]\nkey_tr = matplotlib.lines.Line2D([], [], color='tab:pink', linestyle='-', marker='s', label='truth')\nkey_nn = matplotlib.lines.Line2D([], [], color='k',    linestyle='-', label='independent data')\nkey_gp = matplotlib.lines.Line2D([], [], color='C0',   linestyle='-', label='correlated data')\nax.legend(loc='upper right', handles=[key_tr, key_nn, key_gp]);\n\n\n\n\nThis figure shows more clearly how the first run of the fit, assuming independent data, infers a clearly biased (lower) mass and (higher) concentration. On the other hand, when including a GP in the model to account for correlations in the data the posterior on \\(M_h\\) and \\(c\\) becomes perfectly compatible with the true value within 1-\\(\\sigma\\).\n\n\nPredicted rotation curves of the 2 models\nFinally, let’s plot the predicted rotation curves of the two models in comparison with the data.\n\n# get predictions excluding MCMC warmup phase\npred_wn = sampler_wn.get_samples(group_by_chain=True)['mu'][:,1000:,:].reshape((4000,1000))\npred    = sampler.get_samples(group_by_chain=True)['mu'][:,1000:,:].reshape((4000,1000))\npred_cd = (sampler.get_samples(group_by_chain=True)['mu']+\n           sampler.get_samples(group_by_chain=True)['gp'])[:,1000:,:].reshape((4000,1000))\n\n# get random subset\ninds = np.random.randint(0, 2000, 20)\n\n\nfig,ax = plt.subplots(figsize=(16,6), ncols=3, nrows=2, gridspec_kw={'height_ratios':(0.65,0.35)})\n\ndef commons(ax, i):\n    # rotation curve\n    ax[0,i].plot(x, jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), '--', c='tab:pink', label='true mean')\n    ax[0,i].errorbar(x, vm, yerr=e_vm, fmt='.', c='C0', lw=0.5, label='data')\n    ax[0,i].legend(loc='lower right')\n    ax[0,i].set_ylabel('velocity');\n    # residuals\n    ax[1,i].axhline(y=0, ls='--', c='tab:pink')\n    ax[1,i].errorbar(x, vm-jax_vhalo({'log_mh':lmh, 'log_c':lc}, x), yerr=e_vm, fmt='.', c='C0', lw=0.5)\n    ax[1,i].set_xlabel('radius')\n    ax[1,i].set_ylabel('residuals')\n    ax[1,i].set_ylim(-60,60);\n    \ncommons(ax,0)\ncommons(ax,1)\ncommons(ax,2)\n\nax[0,0].plot(r_grid, pred_wn[inds].T, 'k', alpha=0.1)\nax[0,1].plot(r_grid, pred[inds].T, 'C0', alpha=0.2)\nax[0,2].plot(r_grid, pred_cd[inds].T, 'C2', alpha=0.1)\nax[1,0].plot(r_grid, (pred_wn[inds]-jax_vhalo({'log_mh':lmh, 'log_c':lc}, r_grid)).T, 'k', alpha=0.1)\nax[1,1].plot(r_grid, (pred[inds]-jax_vhalo({'log_mh':lmh, 'log_c':lc}, r_grid)).T, 'C0', alpha=0.2)\nax[1,2].plot(r_grid, (pred_cd[inds]-jax_vhalo({'log_mh':lmh, 'log_c':lc}, r_grid)).T, 'C2', alpha=0.1);\nax[0,0].set_title(\"Model assuming independent data\");\nax[0,1].set_title(\"Model with GP: correlated data\");\nax[0,2].set_title(\"Model with GP \"+r\"$\\rm\\bf conditioned$\"+\" to data\");\n\n\n\n\nIn the left-hand panels we compare 20 random samples of the predicted rotation curve for the first iteration of the model, i.e. the one treating each datapoint as independent. We see that the model rotation curves (in black) tend to overshoot the mean at small radii and tend to fall below it at large radii - this becomes particularly clear in the residuals plot. The reason for this discrepancy is that this model finds a biased result, predicting a higher concentration and a lower mass than the true values.\nThe middle panels similarly compare samples of the predictions of the second model, i.e. the one that uses GPs to model correlations among successive datapoints. We see that while the prediction of the rotation curve (in blue) becomes much less accurate, it is now unbiased. In fact, the true mean rotation curve that we used to generate the dataset is very well encompassed by the random samples of this model.\nThe right panels demonstrate why such a large variety of rotation curve shapes in the blue model is consistent with the dataset. In fact, each blue curve is itself a GP and when conditioning it to the measurements we obtain the green curves in the right panels, which are all consistent with the data (reduced \\(\\chi^2\\) less than unity)."
  },
  {
    "objectID": "posts/2022-03-22-nn-from-scratch.html",
    "href": "posts/2022-03-22-nn-from-scratch.html",
    "title": "Ground-up construction of a simple neural network",
    "section": "",
    "text": "When approaching the study of a new subject I find it extremely useful to get my hands dirty and play around with the stuff I’m learning, in order to cement the knowledge that I’m passively acquiring reading or listening to a lecture. In the case of deep learning, before starting to use massively the superb python libraries available, e.g. pytorch or fast.ai, I think it’s critical to build a simple NN from scratch.\nThe bits required are just linear operations, e.g. matrix multiplications, functional composition and the chain rule to get the derivatives during back-propagation. All of this sounds not terrible at all, so we just need a bit of organization to glue all the pieces together.\nWe take inspiration from the pytorch library and we start by building an abstract Module class.\n\n\nCode\nimport numpy as np\nfrom torch import tensor\nfrom torch import nn\nimport torch, math\nimport random\n\n%config Completer.use_jedi = False\n\nrng = np.random.default_rng()\n\n\n\nclass Module():\n    \"\"\" abstract class: on call it saves the input and output, and it returns the output \"\"\"\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n\nWhen called, Module stores the input and the output items and just returns the output which is defined by the method forward, which needs to be overridden by the derived class. Another method, backward, will have to return the derivative of the function, thus implementing the necessary step for back-propagation.\nLet’s now use the class Module to implement a sigmoid activation function:\n\nsig = lambda x: 1.0/(1.0+np.exp(-x))\n    \nclass Sigmoid(Module):\n    def forward(self, inp): return sig(inp)\n    def bwd(self, out, inp): inp.g = sig(inp) * (1-sig(inp)) * out.g\n\nHere the class Sigmoid inherits from Module and we just need to specify the forward method, which is just the value of the sigmoid function, and the bwd method, which is what is called by backward. We use bwd to implement the derivative of the sigmoid \\[\n\\sigma'(x) = \\sigma(x)\\left[1-\\sigma(x)\\right],\n\\] which we store in the .g attribute, that stands for gradient, of the input. This storing the gradient of the class in the .g attribute of the input combined with the last multiplication by out.g that we do in the bwd method is basically the chain rule. The gradient in each layer of an NN is, according to the chain rule, the derivative of the layer times the derivative of the input. Once computed, we store this in the gradient of inp, which is exactly the same variable as out of the previous layer, thus we can reference its gradient with out.g when climbing back the hierarchy of layers.\nSimilarly, a linear layer \\(W{\\bf x} + b\\), where \\(w\\) is a matrix, \\({\\bf x}\\) is a vector and \\(b\\) is a scalar, can be written as:\n\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n        \n    def forward(self, inp): return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ out.g\n        self.b.g = out.g.sum(0)\n\nAs before, forward implements the linear layer (@ is the matrix multiplication operator in pytorch) and bwd implements the gradient. The derivative of a matrix multiplication \\(W{\\bf x}\\) is just a matrix multiplication by the transpose of the matrix, \\(W^T\\). Since the linear layer has the weights \\(w\\) and bias \\(b\\) parameters that we want to learn, then we need to calculate the gradient of the output of the layer with respect to the weights and the bias. This is what is implemented in self.w.g and self.b.g.\nFinally we can define the loss as a class derived from Module as:\n\nclass Mse(Module):\n    def forward (self, inp, target): return (inp.squeeze(-1) - target).pow(2).mean()\n    def bwd(self, out, inp, target): inp.g = 2*(inp.squeeze(-1)-target).unsqueeze(-1) / target.shape[0]\n\nThis is a mean squared error loss function, \\(L({\\bf y},{\\bf y}_{\\rm target}) = \\sum_i (y_i-y_{i,\\rm target})^2\\), where the forward and bwd methods have the same meaning as above. Notice that here the bwd method just stores the inp.g attribute and does not have a multiplication by out.g, because this is the final layer of our NN.\nFinally we can bundle everything together in a Model class which takes as input a list of layers and implements a forward method, where maps the input into each layer sequentially, and a backward method, where it goes through the gradient of each layer in reversed order.\n\nclass Model():\n    def __init__(self, layers):\n        self.layers = layers\n        self.loss = Mse()\n        \n    def __call__(self, x, target): return self.forward(x, target)\n        \n    def forward(self, x, target):\n        for l in self.layers: x = l(x)\n        return self.loss(x, target)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nLet’s now take some fake data and let’s randomly initialize the weights and biases (unsing standard Xavier initialization so that the output of the layers are still a null mean and unit variance)\n\nn,m = 200,1\n\nx = torch.randn(n,m)\ny = x.pow(2)\n\nnh = 100\n# standard xavier init\nw1 = torch.randn(m,nh)/math.sqrt(m)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)/math.sqrt(nh)\nb2 = torch.zeros(1)\n\nWe can now define a model as a sequence of linear and activation layers and we can make a forward pass to calculate the loss…\n\nmodel = Model([Lin(w1,b1), Sigmoid(), Lin(w2,b2)])\nloss = model(x, y)\n\n…and also a backward pass to calculate the gradients\n\nmodel.backward()\n\nThe architecture above is basically equivalent to an nn.Sequential model\n\nnn.Sequential(nn.Linear(m,nh), nn.Sigmoid(), nn.Linear(nh,1))\n\nSequential(\n  (0): Linear(in_features=1, out_features=100, bias=True)\n  (1): Sigmoid()\n  (2): Linear(in_features=100, out_features=1, bias=True)\n)"
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html",
    "href": "posts/2023-01-13-diffusion_intro.html",
    "title": "Intro to diffusion models from scratch",
    "section": "",
    "text": "During the end-of-the-year break I decided to redo the awesome fast.ai course just to refresh some ideas and to keep up to date. In particular, I knew that this year they were redoing the part 2 of the course, i.e. the deep learning foundations part, which is a fantastic resource to get a profound understanding of deep learning methods.\nI was very pleseantly surprised to see this year they were going from the deep learning foundations to stable diffusion. This got me excited since I felt like this was a great opportunity to learn about the popular diffusion models, which are state-of-the-art in computer vision, to reproduce them and master them myself.\nSo, in this notebook I cover the first steps to set up a simple diffusion model from scratch! I am going to: - generate some data (galaxy rotation curves), - add some random amount of noise to the data, - train an autoencoder on the noisy dataset, this is effectively a denoising network, - sampling some realistic rotation curves gradually denoising pure noise.\nThe main resources I am using for this are the original paper (Sohl-Dickstein et al. 2015) and the DDPM paper, on which stable diffusion and others are built upon, the notebooks of the fast.ai course part 2 and the diffusion models class by huggingface.\n\n#collapse-hide\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.special import i0, i1, k0, k1\nfrom torch import tensor\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch, math\nimport random\n\n%config Completer.use_jedi = False\n%matplotlib inline\n\nrng = np.random.default_rng()"
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html#data-generation",
    "href": "posts/2023-01-13-diffusion_intro.html#data-generation",
    "title": "Intro to diffusion models from scratch",
    "section": "Data generation",
    "text": "Data generation\nFirst of all, let’s generate some data that we can later use for training. I’m going to use the same framework that I’ve been using for the blog posts on autoencoders and VAEs, so it’s entirely skippable if you’ve seen these previous posts.\n\n#collapse-hide\nG, H, Dc = 4.301e-9, 70, 200.\n\ndef fc(x):\n    return np.log(1+x)-x/(1+x)\n\ndef Vvir(Mh):\n    return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.))\n\ndef Rvir(Mh):\n    rho_c = 3. * (H)**2 / (8. * np.pi * G)\n    rho_hat = 4. / 3. * np.pi * Dc * rho_c\n    return 1e3 * np.power(Mh / rho_hat, 1./3.)\n\n\n#collapse-hide\n# halo concentration--mass relation\ndef c(Mh, w_scatter=False, H=70.): \n    if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh)))\n    return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12))\n\n# disc mass--size relation\ndef getRd_fromMd(Md, w_scatter=False):\n    ''' approximate mass-size relation '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.3+0.5)\n\n# disc mass--halo mass relation\ndef getMh_fromMd(Md, w_scatter=False):\n    ''' approximate SHMR '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.75+12.0)\n\nBasically I’m going to generate a dataset of rotation curves of galaxies of different stellar disc masses, dark matter halo masses, disc scale-lengths and halo concentrations. These 4 parameters are not taken at random, but they are constrained by observed galaxy scaling relations. In this way, only the stellar mass is distributed uniformly, while the other parameters are well constrained.\n\nnsamp = 1000\nms = 10**rng.uniform(9, 12, nsamp)\nrd = getRd_fromMd(ms, w_scatter=True)\nmh = getMh_fromMd(ms, w_scatter=True)\ncc = c(mh, w_scatter=True)\n\nFor each tuple of parameters I construct a rotation curve with the following class:\n\nclass curveMod():\n    def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)):\n        self.G, self.H, self.Dc = 4.301e-9, 70, 200.  # physical constants\n        self.Md, self.Rd = Md, Rd\n        self.Mh, self.cc = Mh, cc\n        self.rad = rad\n        \n        if hasattr(self.Md, '__len__'):\n            self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))]\n            self.vdm   = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))]\n            self.vc    = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))]\n        else:\n            self.vdisc = self._vdisc(self.rad, self.Md, self.Rd)\n            self.vdm   = self._vhalo(self.rad, self.Mh, self.cc)\n            self.vc    = np.sqrt(self.vdisc**2+self.vdm**2)\n        \n    def _fc(self, x): return np.log(1+x)-x/(1+x)\n    def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.))\n    def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.)\n    \n    def _vhalo(self, R, Mh, cc):\n        # circular velocity of the halo component (NFW model)\n        rv = self._Rvir(Mh)\n        return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) \n    \n    def _vdisc(self, R, Md, Rd):\n        # circular velocity of the disc component (exponential disc)\n        y = R/2./Rd\n        return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y))))\n\n\ncm=curveMod(ms,rd,mh,cc)\n\nThe final sample of rotation curves that I get can be visualized as follows:\n\nfor v in cm.vc: plt.plot(cm.rad, v)\nplt.xlabel('radius')\nplt.ylabel('velocity');\n\n\n\n\n\nNormalization and train/valid splitting\nAs always when dealing with neural networks, it is generally a good idea to normalize the dataset and to randomly shuffle it.\n\ndef datanorm(x):  return (x-x.mean())/x.std(), x.mean(), x.std()\ndef datascale(x, m, s): return x*s+m\n\nidshuff = torch.randperm(nsamp)\nxdata = tensor(cm.vc, dtype=torch.float)[idshuff,:]\nxdata, xmean, xstd = datanorm(xdata)\n\nI also split the dataset into training and validation sets.\n\nfval = 0.20\nxtrain = xdata[:int(nsamp*(1.0-fval))]\nxvalid = xdata[int(nsamp*(1.0-fval)):]"
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html#adding-noise",
    "href": "posts/2023-01-13-diffusion_intro.html#adding-noise",
    "title": "Intro to diffusion models from scratch",
    "section": "Adding noise",
    "text": "Adding noise\nWe now add noise to our rotation curves, since we want the neural network to learn to de-noise a noisy curve. To do this, we construct a simple noise scheduler which is a function that adds noise to our input dataset corresponding to an amount a \\(\\in [0,1]\\), where if a=0 the curve is noise-free and if a=1 the data is pure Gaussian noise.\nThe term scheduler refers to the arbitrary interpolating function that we choose between the two regimes a=0 and a=1. For the sake of simplicity, here I chose a linear scheduler.\n\ndef add_noise(x, a):\n    noise = torch.randn_like(x)\n    if a.shape != torch.Size([]): a = a.view(-1, 1)\n    return x*(1-a) + noise*a\n\nWe can see the effect of adding noise to a given rotation curve in the set of plots below, where I gradually increase a.\n\nfig,ax=plt.subplots(figsize=(18,3),ncols=5)\n\ndef commons(ax, a):\n    ax.plot(cm.rad, datascale(add_noise(xtrain[1],tensor(a)), xmean, xstd), '.', label='a=%1.1f' % a)\n    ax.plot(cm.rad, datascale(xtrain[1], xmean, xstd))\n    ax.set_xlabel('radius / kpc')\n    ax.legend()\n    ax.set_ylim(-100,400)\n\ncommons(ax[0], 0.1)\nax[0].set_ylabel('velocity / km/s')\ncommons(ax[1], 0.3)\ncommons(ax[2], 0.5)\ncommons(ax[3], 0.7)\ncommons(ax[4], 1.0)"
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html#model",
    "href": "posts/2023-01-13-diffusion_intro.html#model",
    "title": "Intro to diffusion models from scratch",
    "section": "Model",
    "text": "Model\nLet’s now set up the neural network that will learn to de-noise our rotation curves. This is done with a slightly modified autoencoder model, where I’ve added skip connections, just to be closer to the U-Net framework which is popular for diffusion models.\n\nclass AE_net(nn.Module):\n    def __init__(self, ninp, **kwargs):\n        super().__init__()\n        self.encoder_layers = nn.ModuleList([\n            nn.Linear(in_features=ninp, out_features=32),\n            nn.Linear(in_features=32,   out_features=16),\n            nn.Linear(in_features=16,   out_features=4)\n        ])\n        self.decoder_layers = nn.ModuleList([\n            nn.Linear(in_features=4,    out_features=16),\n            nn.Linear(in_features=16,   out_features=32),\n            nn.Linear(in_features=32,   out_features=ninp)\n        ])\n        self.act = nn.SiLU()\n        \n            \n    def forward(self, x):\n        h = [] # skip connections\n        \n        for i, l in enumerate(self.encoder_layers):\n            x = self.act(l(x))\n            if i < 2: h.append(x) # store for skip connection, for all but final layer\n              \n        for i, l in enumerate(self.decoder_layers):\n            if i > 0: x += h.pop() # get stored output for skip connection, for all but first layer\n            x = self.act(l(x)) if i<2 else l(x) # final layer without activation\n            \n        return x\n\n\nmodel = AE_net(len(cm.rad))\n\nAnd now the training phase. Notice that we first add a random amount of noise to the rotation curve dataset and we pass these noisy curves to the autoencoder. This way, the model will learn to recognize the curves even when noise is added to them.\n\n# Adam and MSE Loss\nloss_func = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(2001):\n    \n    # generate noise with random amount\n    noise = torch.rand(xtrain.shape[0])\n    \n    # add noise to data\n    x_noisy = add_noise(xtrain, noise)\n    \n    # prediction\n    ymod = model.forward(x_noisy)\n    \n    # loss\n    loss = loss_func(xtrain, ymod)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch%100==0: print(epoch,\"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % loss_func(xvalid, model.forward(xvalid)))\n\n0 train L:1.00e+00   valid L:8.09e-01\n100 train L:2.18e-01   valid L:5.60e-02\n200 train L:1.44e-01   valid L:6.59e-02\n300 train L:1.58e-01   valid L:4.65e-02\n400 train L:1.56e-01   valid L:4.83e-02\n500 train L:1.69e-01   valid L:2.14e-02\n600 train L:1.51e-01   valid L:3.40e-02\n700 train L:1.80e-01   valid L:3.54e-02\n800 train L:1.86e-01   valid L:1.13e-01\n900 train L:1.56e-01   valid L:4.06e-02\n1000 train L:1.61e-01   valid L:2.52e-02\n1100 train L:1.45e-01   valid L:2.53e-02\n1200 train L:1.39e-01   valid L:2.08e-02\n1300 train L:1.89e-01   valid L:2.77e-02\n1400 train L:1.93e-01   valid L:8.97e-02\n1500 train L:1.91e-01   valid L:2.62e-02\n1600 train L:1.56e-01   valid L:2.71e-02\n1700 train L:1.92e-01   valid L:2.74e-02\n1800 train L:1.41e-01   valid L:2.08e-02\n1900 train L:1.58e-01   valid L:3.61e-02\n2000 train L:1.61e-01   valid L:2.42e-02"
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html#predictions",
    "href": "posts/2023-01-13-diffusion_intro.html#predictions",
    "title": "Intro to diffusion models from scratch",
    "section": "Predictions",
    "text": "Predictions\nHow well does the model predict the underlying rotation curves from their noisy version? It depends a lot from the noise amount. This is rather intuitive, since for low amount of noise we do expect the model to provide almost perfect predictions, whereas when the data is mostly comprised of noise than signal it is not surprising to see the model failing.\nLet’s see this in action with the following plot:\n\nfig,ax=plt.subplots(figsize=(18,3),ncols=5)\n\nx = xtrain[:5]\nnoise = tensor([0.1, 0.2, 0.5, 0.7, 0.9])\nx_noisy = add_noise(x, noise)\n\nwith torch.no_grad():\n    ymod = model(x_noisy).detach()\n\nfor i, xi in enumerate(x):\n    ax[i].plot(cm.rad, datascale(xi, xmean, xstd), c='grey', label='input data')\n    ax[i].plot(cm.rad, datascale(x_noisy[i], xmean, xstd), '.', label='noisy data')\n    ax[i].plot(cm.rad, datascale(ymod[i], xmean, xstd), '-', lw=2, label='model prediction')\n    ax[i].set_xlabel('radius / kpc')\n    ax[i].set_title('a=%1.1f' % noise[i])\n    if i == 0:\n        ax[i].set_ylabel('velocity / km/s');\n        ax[i].legend(loc='lower right')\n        ax[i].set_title('noise amount:   a=%1.1f' % noise[i])\n        \n\n\n\n\nThese results tell us that the model has successfully learned how to denoise high signal-to-noise rotation curves, i.e. data whose noise amount is fairly low, and that it struggels to capture the details of the curve at low signal-to-noise.\nOf course, the autoencoder model that I used is quite simple and it could be further improved by making the model more sophisticated and the noise scheduler more efficient."
  },
  {
    "objectID": "posts/2023-01-13-diffusion_intro.html#sampling",
    "href": "posts/2023-01-13-diffusion_intro.html#sampling",
    "title": "Intro to diffusion models from scratch",
    "section": "Sampling",
    "text": "Sampling\nFinally, let’s have a look at how we can use the model that I’ve just trained to generate new rotation curve data starting from random noise. We could, in principle, simply just feed some random noise into the model since it will give us a rotation curve as output. However, we saw before that the model becomes quite unreliable when the noise is dominant over the signal. So, how do we deal with this?\nA smart, but simple, solution is to borrow from the way a diffusion differential equation is usually solved, that is applying the denoising iteratively in many steps, instead of all in one step. The idea is to start from random, apply just a small step of the denoising network, then take this output and apply another small denoising step to it until we have fully denoised the dataset. In this way, the denoising is done gradually, which allows for complex features to arise in the rotation curve, instead of always just predicting an average curve.\nLet’s apply 30 steps of denoising to 5 initial completely random datasets.\n\nnsteps = 31\nx_ini = torch.randn_like(x)\ninputs  = []\noutputs = []\n\nfor i in range(nsteps):\n    with torch.no_grad():\n        ymod = model(x_ini)\n        \n    if i%10==0:\n        inputs.append(x_ini)\n        outputs.append(ymod) \n        \n    mix_factor = 1/(nsteps - i)                    # how much denoising we apply\n    x_ini = x_ini*(1-mix_factor) + ymod*mix_factor # mix noisy input with model prediction\n    \ninputs  = torch.stack(inputs)\noutputs = torch.stack(outputs)\n\nLet’s now plot a few of these timesteps for the 5 noise inputs:\n\nfig, ax = plt.subplots(figsize=(16,16), nrows=inputs.shape[1], ncols=inputs.shape[0], gridspec_kw={'hspace':0.4})\n\nfor i in range(inputs.shape[1]):\n    for j in range(inputs.shape[0]):\n        ax[i,j].plot(cm.rad, datascale(inputs[j,i], xmean, xstd), '.')\n        ax[i,j].plot(cm.rad, datascale(outputs[j,i], xmean, xstd), '-', lw=2)\n        \n        if i == 0: ax[i,j].set_ylim(-10, 350)\n        if i == 1: ax[i,j].set_ylim(0, 250)\n        if i == 2: ax[i,j].set_ylim(-100, 300)\n        if i == 3: ax[i,j].set_ylim(-50, 300)\n        if i == 4: ax[i,j].set_ylim(-100, 600)\n        \n        if j == 0: ax[i,j].set_ylabel('velocity / km/s')\n        if i == inputs.shape[1]-1: ax[i,j].set_xlabel('radius / kpc')\n        if i == 0: ax[i,j].set_title('step=%d' % (j*10), fontsize=14)\n\n\n\n\nAs we can see, with this procedure the model is able to generate some new rotation curves that have significantly different shapes starting from full random noise! This happens since after the autoencoder has been trained to denoise a real rotation curve database, the slow denoising pipeline based on small timesteps that we created allows the model to enhance some peculiar – and non average – feature of the curve that it has inferred by chance from random noise. We observe the model to slowly convince itself that some accumulation of points that happens by chance actually hides some signal, which is cleaned and enhanced at each timestep.\nNext we will see how to guide the model to see some particular feature in the random noise that we want in our rotation curve output."
  },
  {
    "objectID": "posts/2022-10-07-variational-autoencoder-rotcurves.html",
    "href": "posts/2022-10-07-variational-autoencoder-rotcurves.html",
    "title": "Variational Autoencoder: learning an underlying distribution and generating new data",
    "section": "",
    "text": "This notebook deals with generating an Autoencoder model to learn the underlying distribution of the data. To do this we have to modify the autoencoder such that the encoder does not learn a compressed representation of the input data, but rather it will learn the parameters of the distribution of the data in the latent (compressed) space.\nSo the idea is to start from an observed sample of the distribution of the data \\(P({\\bf X})\\) and to pass this to the encoder which will reduce its dimensionality, i.e. \\(P({\\bf X})\\mapsto P'({\\bf X}_{\\rm c})\\) where \\({\\bf X}\\in\\mathrm{R}^m\\) and \\({\\bf X}_{\\rm c}\\in\\mathrm{R}^n\\) with \\(n<m\\). In other words, in a VAE the encoder step does not represent the input data \\({\\bf X}\\) with a code \\({\\bf X}_{\\rm c}\\), but rather the initial data distribution \\(P({\\bf X})\\) with a compressed distribution \\(P'({\\bf X}_{\\rm c})\\), which we usually need to approximate in some analytic form, e.g. a multi-variate normal \\(P'({\\bf X}_{\\rm c})\\sim \\mathcal{N}(\\mu,\\Sigma)\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom scipy.special import i0, i1, k0, k1\nfrom torch import tensor\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch, math\nimport random\nimport corner\n\n%config Completer.use_jedi = False\n%matplotlib inline\n\nrng = np.random.default_rng()\n\n\n\n\nCode\nG, H, Dc = 4.301e-9, 70, 200.\n\ndef fc(x):\n    return np.log(1+x)-x/(1+x)\n\ndef Vvir(Mh):\n    return np.sqrt((Dc*(H)**2/2)**(1./3.) * (G*Mh)**(2./3.))\n\ndef Rvir(Mh):\n    rho_c = 3. * (H)**2 / (8. * np.pi * G)\n    rho_hat = 4. / 3. * np.pi * Dc * rho_c\n    return 1e3 * np.power(Mh / rho_hat, 1./3.)\n\n\n\n\nCode\n# halo concentration--mass relation\ndef c(Mh, w_scatter=False, H=70.): \n    if w_scatter: return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12) + rng.normal(0.0, 0.11, len(Mh)))\n    return 10.**(0.905 - 0.101 * (np.log10(Mh*H/100.)-12))\n\n# disc mass--size relation\ndef getRd_fromMd(Md, w_scatter=False):\n    ''' approximate mass-size relation '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.3+0.5 + rng.normal(0.0, 0.4, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.3+0.5)\n\n# disc mass--halo mass relation\ndef getMh_fromMd(Md, w_scatter=False):\n    ''' approximate SHMR '''\n    if w_scatter: return 10**((np.log10(Md)-10.7)*0.75+12.0 + rng.normal(0.0, 0.25, len(Md)))\n    return 10**((np.log10(Md)-10.7)*0.75+12.0)\n\n\n\n\nCode\nclass curveMod():\n    def __init__(self, Md, Rd, Mh, cc, rad=np.logspace(-1, np.log10(50), 50)):\n        self.G, self.H, self.Dc = 4.301e-9, 70, 200.  # physical constants\n        self.Md, self.Rd = Md, Rd\n        self.Mh, self.cc = Mh, cc\n        self.rad = rad\n        \n        if hasattr(self.Md, '__len__'):\n            self.vdisc = [self._vdisc(self.rad, self.Md[i], self.Rd[i]) for i in range(len(self.Md))]\n            self.vdm   = [self._vhalo(self.rad, self.Mh[i], self.cc[i]) for i in range(len(self.Md))]\n            self.vc    = [np.sqrt(self.vdisc[i]**2+self.vdm[i]**2) for i in range(len(self.Md))]\n        else:\n            self.vdisc = self._vdisc(self.rad, self.Md, self.Rd)\n            self.vdm   = self._vhalo(self.rad, self.Mh, self.cc)\n            self.vc    = np.sqrt(self.vdisc**2+self.vdm**2)\n        \n    def _fc(self, x): return np.log(1+x)-x/(1+x)\n    def _Vvir(self, Mh): return np.sqrt((self.Dc*(self.H)**2/2)**(1./3.) * (self.G*Mh)**(2./3.))\n    def _Rvir(self, Mh): return 1e3 * (Mh / (0.5*self.Dc*self.H**2 /self.G))**(1./3.)\n    \n    def _vhalo(self, R, Mh, cc):\n        # circular velocity of the halo component (NFW model)\n        rv = self._Rvir(Mh)\n        return np.sqrt(self._Vvir(Mh)**2*rv/R*self._fc(cc*R/rv)/self._fc(cc)) \n    \n    def _vdisc(self, R, Md, Rd):\n        # circular velocity of the disc component (exponential disc)\n        y = R/2./Rd\n        return np.nan_to_num(np.sqrt(2*4.301e-6*Md/Rd*y**2*(i0(y)*k0(y)-i1(y)*k1(y))))\n\n\nLet’s start again by generating the distribution of physical parameters and calculating the rotation curve of a galaxy with those parameters. This part is taken from the blog post on Autoencoders so I just refer to that for details.\n\nnsamp = 2000\nms = 10**rng.uniform(9, 12, nsamp)\nrd = getRd_fromMd(ms, w_scatter=True)\nmh = getMh_fromMd(ms, w_scatter=True)\ncc = c(mh, w_scatter=True)\n\n\ncm=curveMod(ms,rd,mh,cc)\n\n\nfor v in cm.vc: plt.plot(cm.rad, v)\nplt.xlabel('radius')\nplt.ylabel('velocity');\n\n\n\n\nThis plot shows a random realization of nsamp rotation curves from our physical model. These curves are the dataset the we are going to use to train our Variational Autoencoder. Let’s start by normalizing the data and defining the training and validation sets.\n\ndef datanorm(x):  return (x-x.mean())/x.std(), x.mean(), x.std()\ndef datascale(x, m, s): return x*s+m\n\nidshuff = torch.randperm(nsamp)\nxdata = tensor(cm.vc, dtype=torch.float)[idshuff,:]\nxdata, xmean, xstd = datanorm(xdata)\n\nfval = 0.20\nxtrain = xdata[:int(nsamp*(1.0-fval))]\nxvalid = xdata[int(nsamp*(1.0-fval)):]\n\n/Users/lposti/anaconda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484744261/work/torch/csrc/utils/tensor_new.cpp:204.)\n  \"\"\""
  },
  {
    "objectID": "posts/2022-10-07-variational-autoencoder-rotcurves.html#the-variational-autoencoder",
    "href": "posts/2022-10-07-variational-autoencoder-rotcurves.html#the-variational-autoencoder",
    "title": "Variational Autoencoder: learning an underlying distribution and generating new data",
    "section": "The Variational Autoencoder",
    "text": "The Variational Autoencoder\nWe now build the class, deriving from nn.Module, for our Variational AutoEncoder (VAE). In particular, we define the layers in the __init__ method and we define an encoder and a decoder method, just as we did for the Autoencoder. However, this class is quite a lot richer than the one we used for and Autoencoder and we will go through each method below.\n\nclass VariationalAutoEncoder(nn.Module):\n    def __init__(self, ninp, **kwargs):\n        super().__init__()\n        self.encodeLayer1 = nn.Sequential(nn.Linear(in_features=ninp, out_features=32), nn.ReLU())\n        self.encodeLayer2 = nn.Sequential(nn.Linear(in_features=32,   out_features=16), nn.ReLU())\n        self.encodeOut    = nn.Linear(in_features=16,   out_features=8)\n        self.decodeLayer1 = nn.Sequential(nn.Linear(in_features=4,    out_features=16), nn.ReLU())\n        self.decodeLayer2 = nn.Sequential(nn.Linear(in_features=16,   out_features=32), nn.ReLU())\n        self.decodeOut    = nn.Linear(in_features=32,   out_features=ninp)\n        self.ELBO_loss = None\n        \n    def encoder(self, x):       \n        mean, logvar = torch.split(self.encodeOut(self.encodeLayer2(self.encodeLayer1(x))),4,dim=1)\n        return mean, logvar\n    \n    def decoder(self, encoded): return self.decodeOut(self.decodeLayer2(self.decodeLayer1(encoded)))\n    \n    def reparametrize(self, mean, logvar):\n        eps = tensor(rng.normal(size=mean.shape), dtype=torch.float)\n        return eps * torch.exp(logvar * 0.5) + mean # exp(0.5logvar) = std\n    \n    # https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n    # https://arxiv.org/pdf/1312.6114.pdf\n    # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes?noredirect=1&lq=1\n    def _ELBO(self, x, decoded, mean, logvar):\n        mseloss = nn.MSELoss(reduction='sum')\n        logpx_z = -mseloss(x, decoded)\n        KLdiv = -0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim = 1)\n        return (KLdiv - logpx_z).mean()\n        \n    def forward(self, x):\n        mean, logvar = self.encoder(x)\n        z = self.reparametrize(mean, logvar)\n        decoded = self.decoder(z)\n        self.ELBO_loss = self._ELBO(x, decoded, mean, logvar)\n        \n        return decoded\n    \n    def getELBO_loss(self, x):\n        mean, logvar = self.encoder(x)\n        z = self.reparametrize(mean, logvar)\n        decoded = self.decoder(z)\n        return self._ELBO(x, decoded, mean, logvar)\n        \n\nOk, so there’s a lot to break down here!\n\nFirst of all, we notice that the overall structure of the encoder/decoder network is relatively similar to the autoencoder we saw before, with the important difference that the encoder now returns 8 parameters instead of 4. These are the parameters of the multi-variate normal distribution with which we represent the 4-dimensional latent space, so mean and variance for each of the four physical properties that generate the rotation curves. Thus, the encoder step does not output a code, but means and variances for each physical property.\nthe decoder step is instead totally similar to a simple autoencoder and in fact it requires a code as an input. In order to generate a code from the means and variances that come out of the encoder phase without breaking the backprop flow of the algorithm, Kingma & Welling (2013) proposed to use a reparametrization trick, which consists of throwing a new sample from a standard normal and then shifting this to have the same mean and variance as given by the encoder.\nthe forward method of this class follows these steps: the encoder gives means and variances of the latent space, the reparametrization trick is used to generate a code, which is finally decoded by the decoder.\nthe appropriate loss function for a variational autoencoder is the Evidence Lower BOund (ELBO). In fact, minimising the -ELBO means maximising a lower bound on the evidence or likelihood of the model. The evidence, or reconstruction loss, is logpx_z which is just an MSE loss on the data and the decoded output of the autoencoder. This term encourages the reconstruction of the dataset and tends to prefer separated encodings for each element of the dataset. The other term, KLdiv, is the Kullback-Leibler divergence of the proposed distribution in the latent space with the likelihood. This term has the opposite effect of promoting overlapping encodings for separate observations. For this reason, maximising ELBO guarantees to achieve a nice compromise between representing the original data and the ability to generalize by generating realistic new data.\n\nWith these changes our neural network is now capable of constructing an approximation for the distribution of the four physical parameters in the latent space. We can now run the usual optimization algorithm and start training this model.\n\nvae = VariationalAutoEncoder(len(cm.rad))\n\n# Adam and ELBO Loss\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-2)\n\nfor epoch in range(2000):\n    ymod = vae.forward(xtrain)\n    loss = vae.ELBO_loss\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    optimizer.zero_grad()\n    \n#     print (epoch, \"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % vae.getELBO_loss(xvalid))\n    if epoch%100==0: print (epoch, \"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % vae.getELBO_loss(xvalid))\n\n0 train L:8.46e+04   valid L:1.76e+04\n100 train L:1.29e+03   valid L:3.67e+02\n200 train L:5.40e+02   valid L:1.73e+02\n300 train L:4.72e+02   valid L:1.39e+02\n400 train L:1.42e+02   valid L:6.75e+01\n500 train L:1.19e+02   valid L:5.85e+01\n600 train L:9.75e+01   valid L:5.35e+01\n700 train L:1.05e+02   valid L:5.57e+01\n800 train L:7.98e+01   valid L:4.68e+01\n900 train L:1.91e+02   valid L:7.53e+01\n1000 train L:7.49e+01   valid L:4.26e+01\n1100 train L:8.18e+01   valid L:4.40e+01\n1200 train L:6.40e+01   valid L:3.92e+01\n1300 train L:6.89e+01   valid L:3.86e+01\n1400 train L:6.15e+01   valid L:3.69e+01\n1500 train L:6.42e+01   valid L:3.66e+01\n1600 train L:5.76e+01   valid L:3.55e+01\n1700 train L:5.99e+01   valid L:3.60e+01\n1800 train L:1.28e+02   valid L:5.11e+01\n1900 train L:4.93e+01   valid L:3.26e+01\n\n\n\nfor v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy())\nplt.xlabel('radius')\nplt.ylabel('velocity');\n\n\n\n\nThe plot above shows the distribution of rotation curves that the VAE has learned. We can see that there is a quite large variety of rotation curve shapes that can be represented by this model. Notice that the region in the radius-velicity space covered by the VAE is indeed quite similar to that of the training set (see plot above).\nThis shows that the VAE has indeed learned an effective distribution in the latent space which generates the rotation curve dataset we started from.\n\nExploring the latent space distribution\nWe can now have a look at what the VAE has learned about the latent space. To do so, we can take the means and variances derived by the encoder on the training set and we can use them to generate samples on the latent space. Basically for each \\(x_i\\) in the training set we get a \\(\\mu_i\\) and a \\(\\sigma^2_i\\) and we draw 100 samples from \\(\\mathcal{N}(\\mu_i, \\sigma^2_i)\\).\n\nmsm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach()\n# the above are the means and variances obtained by the encoder\n\nns = 100\nsss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0]\nfor i in range(1, msm.shape[0]):\n    sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i]))\nprint (sss.shape)\n\ntorch.Size([160000, 4])\n\n\nWe thus have an array of training_set_size x N_samples = 1600 x 100 = 160000 samples generated from the distribution inferred by the VAE on the latent space. Let’s now plot them (I’m using corner to do so)\n\nrr = ((-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5))\nfig = corner.corner(sss.numpy(), range=rr, hist_kwargs={\"density\":True});\nfig = corner.corner(msm.numpy(), range=rr, color='C1', fig=fig, hist_kwargs={\"density\":True});\n\n\n\n\nHere in black we plotted the resulting samples of the latent space as described above, while in orange we overplot just the means \\(\\mu_i\\) for each element in the training set. It turns out that the distribution of the means (in orange) is virtually identical to that derived from sampling each multi-variate Gaussian in the latent space (in black).\nThis implies that the variances \\(\\sigma^2_i\\) that are the output of the encoder are generally quite small and that the VAE effectively reconstructs the distribution of the latent space by superimposing many thin Gaussians, all with slightly different mean and small variance. In fact, one could interpret the orange distribution as a superposition of Dirac deltas centered at each mean derived by the encoder on the training set, i.e. \\(\\sum_i\\delta_i(x-\\mu_i)\\).\nLet’s now plot together the physical parameters that we used to generate each rotation curve in the training set, together with the means derived by the encoder step on each element of that set. This allows us to explore whether there are any correlations between the original 4 physical parameters and the 4 dimensions of the latent space constructed by the VAE.\nTo do this we can stack together the tensor of physical parameters and that of the means.\n\nmdshuff, mhshuff = [cm.Md[i] for i in idshuff], [cm.Mh[i] for i in idshuff]\nrdshuff, ccshuff = [cm.Rd[i] for i in idshuff], [cm.cc[i] for i in idshuff]\nmdtrain, mhtrain = mdshuff[:int(nsamp*(1.0-fval))], mhshuff[:int(nsamp*(1.0-fval))]\nrdtrain, cctrain = rdshuff[:int(nsamp*(1.0-fval))], ccshuff[:int(nsamp*(1.0-fval))]\n\n# physical parameters corresponding to each element of the training set\npartrain = (np.vstack([mdtrain, mhtrain, rdtrain, cctrain]).T)\n\n# stacking the tensor of phsyical parameters with that of the means derived by the encoder\ndd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy()\n\nrr2 = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-4,6),(-2.5,8.0),(-6,3),(-1.5,2.5))\nll  = ['Mstar', 'Mhalo', 'Rd', 'c', 'L1', 'L2', 'L3', 'L4']\ncorner.corner(dd, range=rr2, smooth=0.75, smooth1d=0.75, labels=ll);\n\n\n\n\nOk, now this corner plot has a lot of information…let’s breakdown the most important ones:\n\nthe first 4 blocks of this corner plot are relative to the physical parameters (ms,mh,rd,cc) and show the marginal distribution of each of these parameters and how they are correlated with each other.\nthe last 4 blocks, instead, are relative to the 4 latent parameters (L1,L2,L3,L4), and again show the marginal distribution of each and their mutual correlations - this is equivalent to the corner plot just above (in that case this was plotted in orange colour)\nthe 4x4 sub-block on the lower-left corner is possibly the most interesting one as it is the “mixed” block that highlights the relation between the physical and latent parameters. Each of these 16 panels show the correlation of one of the physical parameters with one of the latent ones. We can see that there are several significant correlations (e.g. ms-L1, mh-L2, rd-L3), meaning that these two sets are not independent.\n\n\n\nGenerating new realistic data\nNow that we have a working approximation of the distribution in the latent space it is easy to use the VAE to generate new rotation curves. We will showcase now a quite simplistic way to do it, which is by assuming that we can represent the latent space with a 4-dimensional Gaussian, even though we have seen in the plots above that the actual distribution is more complex.\nWe consider the means \\(\\mu_i\\) that the encoder derives for the full training set and we take the mean and standard deviation of these. We use these two parameters to define a normal distribution\n\nsize=500\n\ndd = torch.distributions.normal.Normal(msm.mean(dim=0), msm.std(dim=0))\nnew_code = dd.sample(torch.Size([size]))\n\nLet’s make a comparison by plotting the marginalised distributions of the 4 latent parameters with that of the normal distribution that we are assuming\n\nfig,ax = plt.subplots(figsize=(12,3), ncols=4)\n\nbins=[np.linspace(rr[0][0],rr[0][1],20), np.linspace(rr[1][0], rr[1][1],20), \n      np.linspace(rr[2][0],rr[2][1],20), np.linspace(rr[3][0], rr[3][1],20)]\nfor i in range(4):\n    ax[i].hist(msm[:,i].numpy(), bins=bins[i], density=True, label='data');\n    ax[i].hist(new_code[:,i].numpy(), bins=bins[i], histtype='step', lw=2, density=True, \n               label='$\\mathcal{N}$'+'-approx');\n    if i==0: ax[i].legend(loc='upper right', frameon=False)\n    ax[i].set_xlabel('L%1d'%(i+1))\n\n\n\n\nQuite a big difference!\nHowever, this is not an issue since we are just using a normal distribution since it is convenient to sample and for us it is just a means to generate plausible code to be interpreted by the decoder. As a matter of fact, by doing this we are able to generate quite a new variety of possible galaxy rotation curves.\n\nfig,ax = plt.subplots(figsize=(6,4))\nfor v in datascale(vae.decoder(new_code),xmean,xstd)[:100]: ax.plot(cm.rad, v.detach().numpy())\nax.set_xlabel('radius')\nax.set_ylabel('velocity');\n\n\n\n\nWith a bit on work on the reparametrization trick, we can relax the assumption that the distribution in the latent space is of a multi-variate, but uncorrelated, normal. In practice, instead of having just 4 means and 4 variances as output of the encoder step, we also add 6 covariances, so that we can define the full non-zero covariance matrix of the multi-variate normal in the latent space.\nUnfortunately this require quite a bit more math on the reparametrization trick, which is not anymore just \\(\\epsilon*\\sigma+\\mu\\), but where the full covariance matrix is used. This calculation requires, among other things, to derive the square root of a matrix, for which we employ the SVD decomposition: if \\(A = U\\,{\\rm diag}(s)\\,V^{\\rm T}\\), where \\(A, U, V \\in \\mathbb{R}^{n, n}\\), \\(s\\in\\mathbb{R}^n\\), and diag\\((s)\\) is a diagonal matrix with the elements of \\(s\\) as diagonal, then \\(\\sqrt{A} = U\\,{\\rm diag}\\left(\\sqrt{s}\\right)\\,V^{\\rm T}\\).\n\nclass VariationalAutoEncoder(nn.Module):\n    def __init__(self, ninp, **kwargs):\n        super().__init__()\n        self.encodeLayer1 = nn.Linear(in_features=ninp, out_features=32)\n        self.encodeLayer2 = nn.Linear(in_features=32,   out_features=16)\n        self.encodeOut    = nn.Linear(in_features=16,   out_features=14)\n        self.decodeLayer1 = nn.Linear(in_features=4,    out_features=16)\n        self.decodeLayer2 = nn.Linear(in_features=16,   out_features=32)\n        self.decodeOut    = nn.Linear(in_features=32,   out_features=ninp)\n        self.ELBO_loss = None\n        \n    def encoder(self, x):       \n        mean, logvar, covs = torch.split(self.encodeOut(F.relu(self.encodeLayer2(F.relu(self.encodeLayer1(x))))), \n                                         [4, 4, 6], dim=1)\n        return mean, logvar, covs\n    \n    def decoder(self, encoded): return self.decodeOut(F.relu(self.decodeLayer2(F.relu(self.decodeLayer1(encoded)))))\n    \n    def reparametrize(self, mean, m_cov):\n        eps = tensor(rng.normal(size=mean.shape), dtype=torch.float)\n#         return eps * var.sqrt() + mean\n        \n        # find matrix square root with SVD decomposition\n        # https://math.stackexchange.com/questions/3820169/a-is-a-symmetric-positive-definite-matrix-it-has-square-root-using-svd?noredirect=1&lq=1\n        U,S,V = torch.svd(m_cov)                                             # A       = U diag(S) V.T\n        dS = torch.stack([torch.diag(S[i,:]) for i in range(S.shape[0])])    # sqrt(A) = U diag(sqrt(S)) V.T\n        cov_sqrt = torch.einsum('bij,bkj->bik',torch.einsum('bij,bjk->bik',U,dS.sqrt()),V)\n        return torch.einsum('bij,bi->bj', cov_sqrt, eps) + mean \n    \n    def _ELBO(self, x, decoded, mean, m_cov, var):\n        mseloss = nn.MSELoss(reduction='sum')\n        logpx_z = -mseloss(x, decoded)\n        KLdiv = -0.5 * (torch.log(m_cov.det()) + 4 - torch.sum(mean**2 + var, dim = 1))\n        return torch.mean((KLdiv - logpx_z)[~(KLdiv - logpx_z).isnan()])  # torch.nanmean\n    \n    def _get_m_cov(self, logvar, covs):\n        # covariance matrix\n        m_cov = torch.zeros(logvar.shape[0], 4, 4)\n        m_cov[:,[0,1,2,3],[0,1,2,3]] = logvar.exp()\n        m_cov[:,[0,0,0,1,1,2],[1,2,3,2,3,3]] = covs\n        m_cov[:,[1,2,3,2,3,3],[0,0,0,1,1,2]] = covs\n#         var = torch.einsum('bii->bi', m_cov)\n        return m_cov, logvar.exp()\n        \n    def forward(self, x):\n        mean, logvar, covs = self.encoder(x)\n        m_cov, var = self._get_m_cov(logvar, covs)\n        z = self.reparametrize(mean, m_cov)\n        decoded = self.decoder(z)\n        self.ELBO_loss = self._ELBO(x, decoded, mean, m_cov, var)\n        \n        return decoded\n    \n    def getELBO_loss(self, x):\n        mean, logvar, covs = self.encoder(x)\n        m_cov, var = self._get_m_cov(logvar, covs)\n        z = self.reparametrize(mean, m_cov)\n        decoded = self.decoder(z)\n        return self._ELBO(x, decoded, mean, m_cov, var)\n\n\nvae = VariationalAutoEncoder(len(cm.rad))\n\n# Adam and ELBO Loss\noptimizer = torch.optim.Adam(vae.parameters(), lr=0.4e-2)\n\nfor epoch in range(1000):\n    ymod = vae.forward(xtrain)\n    loss = vae.ELBO_loss\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    optimizer.zero_grad()\n    \n#     print (epoch, \"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % vae.getELBO_loss(xvalid))\n    if epoch%50==0: print (epoch, \"train L:%1.2e\" % loss, \"  valid L:%1.2e\" % vae.getELBO_loss(xvalid))\n\n0 train L:8.26e+04   valid L:2.21e+04\n50 train L:6.80e+03   valid L:1.42e+03\n100 train L:1.47e+03   valid L:4.22e+02\n150 train L:9.56e+02   valid L:2.77e+02\n200 train L:7.22e+02   valid L:1.98e+02\n250 train L:5.99e+02   valid L:2.26e+02\n300 train L:4.56e+02   valid L:1.53e+02\n350 train L:4.18e+02   valid L:1.85e+02\n400 train L:3.63e+02   valid L:1.66e+02\n450 train L:3.53e+02   valid L:1.57e+02\n500 train L:3.33e+02   valid L:1.39e+02\n550 train L:4.32e+02   valid L:1.75e+02\n600 train L:3.07e+02   valid L:1.39e+02\n650 train L:3.10e+02   valid L:1.10e+02\n700 train L:2.77e+02   valid L:1.09e+02\n750 train L:9.70e+02   valid L:3.07e+02\n800 train L:6.58e+02   valid L:3.33e+02\n850 train L:4.97e+02   valid L:2.41e+02\n900 train L:4.53e+02   valid L:1.82e+02\n950 train L:6.56e+02   valid L:3.57e+02\n\n\n\nfor v in datascale(vae.forward(xvalid),xmean,xstd): plt.plot(cm.rad, v.detach().numpy())\nplt.xlabel('radius')\nplt.ylabel('velocity');\n\n\n\n\n\nmsm, lvsm = vae.encoder(xtrain)[0].detach(), vae.encoder(xtrain)[1].detach()\n# the above are the means and variances obtained by the encoder\n\nns = 100\nsss = tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[0] * 0.5) + msm[0]\nfor i in range(1, msm.shape[0]):\n    sss = torch.vstack((sss, tensor(rng.normal(size=(ns, 4)), dtype=torch.float) * torch.exp(lvsm[i] * 0.5) + msm[i]))\nprint (sss.shape)\n\ntorch.Size([160000, 4])\n\n\n\nrr = ((-25,30),(-40,15),(-30,10),(-20,6))\nfig = corner.corner(sss.numpy(), range=rr, hist_kwargs={\"density\":True});\nfig = corner.corner(msm.numpy(), range=rr, color='C1', fig=fig, hist_kwargs={\"density\":True});\n\n\n\n\n\n# stacking the tensor of phsyical parameters with that of the means derived by the encoder\ndd = torch.hstack((torch.from_numpy(np.log10(partrain)), msm)).numpy()\n\nrr = ((9,12),(10.3,13.6),(-1.0,1.7),(0.5,1.4),(-25,30),(-40,15),(-30,10),(-20,6))\nll = ['Mstar', 'Mhalo', 'Rd', 'c', 'L1', 'L2', 'L3', 'L4']\ncorner.corner(dd, range=rr, smooth=0.75, smooth1d=0.75, labels=ll);"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am Lorenzo Posti, currently a postdoctoral researcher at the Astronomical Observatory of Strasbourg (France). Before that, I’ve been doing research in Astrophysics in Groningen (Netherlands), Bologna (Italy), Oxford (UK) and Baltimore (USA).\nYou can find my updated list of publications here or on ADS. I used to maintain a personal website with some details on my research, however I stopped doing that for various reasons. An archived and stripped-down version can be found here.\nYou can reach out to me at lorenzo.posti@gmail.com."
  }
]