{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /basics/jupyter/neural_network/2022/03/22/NN-from-scratch\n",
    "author: Lorenzo Posti\n",
    "branch: master\n",
    "categories:\n",
    "- neural_network\n",
    "- basics\n",
    "- jupyter\n",
    "date: '2022-03-22'\n",
    "description: Constructing a simple multi-layered neural network (NN) from scratch\n",
    "  using pure `python` and a bit of `pytorch`. This is mostly my personal re-writing\n",
    "  of the fantastic lesson 1 of the `fast.ai` [course Part 2](https://course19.fast.ai/videos/?lesson=8)\n",
    "output-file: 2022-03-22-nn-from-scratch.html\n",
    "title: Ground-up construction of a simple neural network\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909a9cb",
   "metadata": {},
   "source": [
    "## Linear layers and activation functions from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4f260",
   "metadata": {},
   "source": [
    "When approaching the study of a new subject I find it extremely useful to get my hands dirty and play around with the stuff I'm learning, in order to cement the knowledge that I'm passively acquiring reading or listening to a lecture. In the case of deep learning, before starting to use massively the superb `python` libraries available, e.g. `pytorch` or `fast.ai`, I think it's critical to build a simple NN from scratch.\n",
    "\n",
    "The bits required are just linear operations, e.g. matrix multiplications, functional composition and the chain rule to get the derivatives during back-propagation. All of this sounds not terrible at all, so we just need a bit of organization to glue all the pieces together.\n",
    "\n",
    "We take inspiration from the `pytorch` library and we start by building an abstract `Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbe5c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "import torch, math\n",
    "import random\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34ce12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    \"\"\" abstract class: on call it saves the input and output, and it returns the output \"\"\"\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c03390",
   "metadata": {},
   "source": [
    "When called, `Module` stores the input and the output items and just returns the output which is defined by the method `forward`, which needs to be overridden by the derived class. Another method, `backward`, will have to return the derivative of the function, thus implementing the necessary step for back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357303d",
   "metadata": {},
   "source": [
    "Let's now use the class `Module` to implement a sigmoid activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "844a1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "    \n",
    "class Sigmoid(Module):\n",
    "    def forward(self, inp): return sig(inp)\n",
    "    def bwd(self, out, inp): inp.g = sig(inp) * (1-sig(inp)) * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03542293",
   "metadata": {},
   "source": [
    "Here the class `Sigmoid` inherits from `Module` and we just need to specify the `forward` method, which is just the value of the sigmoid function, and the `bwd` method, which is what is called by `backward`.\n",
    "We use `bwd` to implement the derivative of the sigmoid\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)\\left[1-\\sigma(x)\\right],\n",
    "$$\n",
    "which we store in the `.g` attribute, that stands for gradient, of the input. This storing the gradient of the class in the `.g` attribute of the input combined with the last multiplication by `out.g` that we do in the `bwd` method is basically the **chain rule**. The gradient in each layer of an NN is, according to the chain rule, the derivative of the layer times the derivative of the input. Once computed, we store this in the gradient of `inp`, which is exactly the same variable as `out` *of the previous layer*, thus we can reference its gradient with `out.g` when climbing back the hierarchy of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19089925",
   "metadata": {},
   "source": [
    "Similarly, a linear layer $W{\\bf x} + b$, where $w$ is a matrix, ${\\bf x}$ is a vector and $b$ is a scalar, can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09d73693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3d447",
   "metadata": {},
   "source": [
    "As before, `forward` implements the linear layer (`@` is the matrix multiplication operator in `pytorch`) and `bwd` implements the gradient. The derivative of a matrix multiplication $W{\\bf x}$ is just a matrix multiplication by the transpose of the matrix, $W^T$.\n",
    "Since the linear layer has the weights $w$ and bias $b$ parameters that we want to learn, then we need to calculate the gradient of the output of the layer with respect to the weights and the bias. This is what is implemented in `self.w.g` and `self.b.g`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260d32f",
   "metadata": {},
   "source": [
    "Finally we can define the loss as a class derived from `Module` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5213ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, target): return (inp.squeeze(-1) - target).pow(2).mean()\n",
    "    def bwd(self, out, inp, target): inp.g = 2*(inp.squeeze(-1)-target).unsqueeze(-1) / target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d7e84",
   "metadata": {},
   "source": [
    "This is a mean squared error loss function, $L({\\bf y},{\\bf y}_{\\rm target}) = \\sum_i (y_i-y_{i,\\rm target})^2$, where the `forward` and `bwd` methods have the same meaning as above. Notice that here the `bwd` method just stores the `inp.g` attribute and *does not have a multiplication by* `out.g`, because this is the final layer of our NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4f968",
   "metadata": {},
   "source": [
    "Finally we can bundle everything together in a `Model` class which takes as input a list of layers and implements a `forward` method, where maps the input into each layer sequentially, and a `backward` method, where it goes through the gradient of each layer in reversed order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d93e255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, target): return self.forward(x, target)\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d13232",
   "metadata": {},
   "source": [
    "Let's now take some fake data and let's randomly initialize the weights and biases (unsing standard [Xavier initialization](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) so that the output of the layers are still a null mean and unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbcf1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = 200,1\n",
    "\n",
    "x = torch.randn(n,m)\n",
    "y = x.pow(2)\n",
    "\n",
    "nh = 100\n",
    "# standard xavier init\n",
    "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c22d2",
   "metadata": {},
   "source": [
    "We can now define a model as a sequence of linear and activation layers and we can make a forward pass to calculate the loss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c8de8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([Lin(w1,b1), Sigmoid(), Lin(w2,b2)])\n",
    "loss = model(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e0059",
   "metadata": {},
   "source": [
    "...and also a backward pass to calculate the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45c3aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "633b4ae5",
   "metadata": {},
   "source": [
    "The architecture above is basically equivalent to an `nn.Sequential` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc7c7134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(nn.Linear(m,nh), nn.Sigmoid(), nn.Linear(nh,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7b6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892be34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
